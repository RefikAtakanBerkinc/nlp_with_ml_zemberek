{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9979653,"sourceType":"datasetVersion","datasetId":6140720}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nimport re\nimport string\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTETomek\n\nfrom sklearn.metrics import (\n    accuracy_score, \n    precision_score, \n    recall_score, \n    f1_score, \n    confusion_matrix,\n    classification_report\n)\n\n\nall_sheets = pd.read_excel(\n    \"/kaggle/input/trke-nefret-sylemleri-veriseti/Turkce Nefret Soylemi Veriseti.xlsx\",\n    sheet_name=None, \n    header=1\n)\nall_sheets_copy = all_sheets.copy()\nall_sheets_copy.pop('TOPLAM', None)  \ndf = pd.concat(all_sheets_copy.values(), ignore_index=True)\n\ndef preprocess_text(text):\n    text = str(text).lower()\n    text = re.sub(r'http\\S+', '', text)   \n    text = re.sub(r'@\\w+', '', text)      \n    text = re.sub(r'#\\w+', '', text)      \n    text = text.translate(str.maketrans('', '', string.punctuation))  \n    text = re.sub(r'\\d+', '', text)       \n    text = text.strip()\n    return text\n\ndf['cleaned_text'] = df['Tweet'].apply(preprocess_text)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df['cleaned_text'],\n    df['Etiket'],\n    test_size=0.2,\n    random_state=42\n)\n\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.transform(y_test)\n\nclass_names = le.classes_\n\nprint(\"Training set shape:\", X_train.shape, y_train.shape)\nprint(\"Test set shape:\", X_test.shape, y_test.shape)\nprint(\"Class names:\", class_names)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:58:52.038424Z","iopub.execute_input":"2024-12-22T08:58:52.038715Z","iopub.status.idle":"2024-12-22T08:58:59.975289Z","shell.execute_reply.started":"2024-12-22T08:58:52.038693Z","shell.execute_reply":"2024-12-22T08:58:59.974569Z"}},"outputs":[{"name":"stdout","text":"Training set shape: (8179,) (8179,)\nTest set shape: (2045,) (2045,)\nClass names: ['hiçbiri' 'nefret' 'saldırgan']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nvectorizers = {\n    'Count Unigram': CountVectorizer(ngram_range=(1, 1)),\n    'Count Bigram': CountVectorizer(ngram_range=(1, 2)),\n    'TFIDF Unigram': TfidfVectorizer(ngram_range=(1, 1)),\n    'TFIDF Bigram': TfidfVectorizer(ngram_range=(1, 2))\n}\n\nnumeric_resampling_methods = {\n    'SMOTE': SMOTE(random_state=42),\n    'RandomUnderSampler': RandomUnderSampler(random_state=42),\n    'SMOTETomek': SMOTETomek(random_state=42),\n    'None': None \n}\n\npreprocessed_data = {}\n\nfor vec_name, vectorizer in vectorizers.items():\n    print(f\"Vektörleştirici uygulanıyor: {vec_name}\")\n    X_train_vec = vectorizer.fit_transform(X_train)\n    X_test_vec = vectorizer.transform(X_test)\n\n    X_train_vec = X_train_vec.astype(np.float32)\n    X_test_vec = X_test_vec.astype(np.float32)\n\n    print(f\"{vec_name} vektörleştirici uygulandı. Eğitim verisi boyutu: {X_train_vec.shape}\")\n\n    for resample_name, resampler in numeric_resampling_methods.items():\n        print(f\"Yeniden örnekleme yöntemi uygulanıyor: {resample_name}\")\n        if resampler is not None:\n            X_res, y_res = resampler.fit_resample(X_train_vec, y_train)\n            print(f\"{resample_name} yöntemi uygulandı. Yeni eğitim verisi boyutu: {X_res.shape}\")\n        else:\n            X_res, y_res = X_train_vec, y_train\n            print(f\"Yeniden örnekleme yapılmadı. Eğitim verisi boyutu: {X_res.shape}\")\n\n        key = (vec_name, resample_name)\n        preprocessed_data[key] = {\n            'X_train': X_res,\n            'X_test': X_test_vec,\n            'y_train': y_res,\n            'y_test': y_test\n        }\n        print(f\"Veri {key} anahtarı ile preprocessed_data'ya eklendi.\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T08:59:07.642290Z","iopub.execute_input":"2024-12-22T08:59:07.642753Z","iopub.status.idle":"2024-12-22T09:15:43.617522Z","shell.execute_reply.started":"2024-12-22T08:59:07.642725Z","shell.execute_reply":"2024-12-22T09:15:43.616542Z"}},"outputs":[{"name":"stdout","text":"Vektörleştirici uygulanıyor: Count Unigram\nCount Unigram vektörleştirici uygulandı. Eğitim verisi boyutu: (8179, 46317)\nYeniden örnekleme yöntemi uygulanıyor: SMOTE\nSMOTE yöntemi uygulandı. Yeni eğitim verisi boyutu: (18585, 46317)\nVeri ('Count Unigram', 'SMOTE') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: RandomUnderSampler\nRandomUnderSampler yöntemi uygulandı. Yeni eğitim verisi boyutu: (414, 46317)\nVeri ('Count Unigram', 'RandomUnderSampler') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: SMOTETomek\nSMOTETomek yöntemi uygulandı. Yeni eğitim verisi boyutu: (18585, 46317)\nVeri ('Count Unigram', 'SMOTETomek') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: None\nYeniden örnekleme yapılmadı. Eğitim verisi boyutu: (8179, 46317)\nVeri ('Count Unigram', 'None') anahtarı ile preprocessed_data'ya eklendi.\n\nVektörleştirici uygulanıyor: Count Bigram\nCount Bigram vektörleştirici uygulandı. Eğitim verisi boyutu: (8179, 183420)\nYeniden örnekleme yöntemi uygulanıyor: SMOTE\nSMOTE yöntemi uygulandı. Yeni eğitim verisi boyutu: (18585, 183420)\nVeri ('Count Bigram', 'SMOTE') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: RandomUnderSampler\nRandomUnderSampler yöntemi uygulandı. Yeni eğitim verisi boyutu: (414, 183420)\nVeri ('Count Bigram', 'RandomUnderSampler') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: SMOTETomek\nSMOTETomek yöntemi uygulandı. Yeni eğitim verisi boyutu: (18585, 183420)\nVeri ('Count Bigram', 'SMOTETomek') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: None\nYeniden örnekleme yapılmadı. Eğitim verisi boyutu: (8179, 183420)\nVeri ('Count Bigram', 'None') anahtarı ile preprocessed_data'ya eklendi.\n\nVektörleştirici uygulanıyor: TFIDF Unigram\nTFIDF Unigram vektörleştirici uygulandı. Eğitim verisi boyutu: (8179, 46317)\nYeniden örnekleme yöntemi uygulanıyor: SMOTE\nSMOTE yöntemi uygulandı. Yeni eğitim verisi boyutu: (18585, 46317)\nVeri ('TFIDF Unigram', 'SMOTE') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: RandomUnderSampler\nRandomUnderSampler yöntemi uygulandı. Yeni eğitim verisi boyutu: (414, 46317)\nVeri ('TFIDF Unigram', 'RandomUnderSampler') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: SMOTETomek\nSMOTETomek yöntemi uygulandı. Yeni eğitim verisi boyutu: (18585, 46317)\nVeri ('TFIDF Unigram', 'SMOTETomek') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: None\nYeniden örnekleme yapılmadı. Eğitim verisi boyutu: (8179, 46317)\nVeri ('TFIDF Unigram', 'None') anahtarı ile preprocessed_data'ya eklendi.\n\nVektörleştirici uygulanıyor: TFIDF Bigram\nTFIDF Bigram vektörleştirici uygulandı. Eğitim verisi boyutu: (8179, 183420)\nYeniden örnekleme yöntemi uygulanıyor: SMOTE\nSMOTE yöntemi uygulandı. Yeni eğitim verisi boyutu: (18585, 183420)\nVeri ('TFIDF Bigram', 'SMOTE') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: RandomUnderSampler\nRandomUnderSampler yöntemi uygulandı. Yeni eğitim verisi boyutu: (414, 183420)\nVeri ('TFIDF Bigram', 'RandomUnderSampler') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: SMOTETomek\nSMOTETomek yöntemi uygulandı. Yeni eğitim verisi boyutu: (18585, 183420)\nVeri ('TFIDF Bigram', 'SMOTETomek') anahtarı ile preprocessed_data'ya eklendi.\n\nYeniden örnekleme yöntemi uygulanıyor: None\nYeniden örnekleme yapılmadı. Eğitim verisi boyutu: (8179, 183420)\nVeri ('TFIDF Bigram', 'None') anahtarı ile preprocessed_data'ya eklendi.\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\ndef print_classification_results(y_true, y_pred, conf_matrix, title=\"Sınıflandırma Sonuçları\"):\n\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    \n    print(title + \":\")\n    print(f\"  Doğruluk (Accuracy): {accuracy:.4f}\")\n    print(f\"  Precision (Ağ.): {precision:.4f}\")\n    print(f\"  Recall (Ağ.)   : {recall:.4f}\")\n    print(f\"  F1-Skor (Ağ.)  : {f1:.4f}\")\n    \n    print(\"\\nKarışıklık Matrisi:\")\n    for row in conf_matrix:\n        print(\"  \" + \" \".join(map(str, row)))\n\ndef extended_classification_report(\n    clf, \n    X_train, y_train, \n    X_test, y_test, \n    target_names, \n    vec_name=\"\", \n    resample_name=\"\", \n    classifier_name=\"\"\n):\n\n    \n    clf.fit(X_train, y_train)\n    \n    y_train_pred = clf.predict(X_train)\n    y_test_pred  = clf.predict(X_test)\n    \n    train_cm = confusion_matrix(y_train, y_train_pred)\n    test_cm  = confusion_matrix(y_test,  y_test_pred)\n    \n    print(\"=\"*80)\n    print(f\"Detaylı Rapor - Vektörleştirici: {vec_name}, Yeniden Örnekleme: {resample_name}, Model: {classifier_name}\")\n    print(\"=\"*80)\n    \n    print(\"\\n[EĞİTİM (TRAIN) SONUÇLARI]\")\n    print_classification_results(\n        y_train, \n        y_train_pred, \n        train_cm, \n        title=\"Eğitim Verisi Sonuçları\"\n    )\n    train_report_dict = classification_report(\n        y_train, \n        y_train_pred, \n        target_names=target_names, \n        output_dict=True, \n        zero_division=0\n    )\n    train_report_df = pd.DataFrame(train_report_dict).transpose()\n    print(\"\\nSınıf bazlı metrikler (EĞİTİM):\")\n    print(train_report_df)\n\n    if hasattr(clf, 'loss_curve_'):\n        print(\"\\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\")\n        print(clf.loss_curve_) \n\n    print(\"\\n[TEST SONUÇLARI]\")\n    print_classification_results(\n        y_test, \n        y_test_pred, \n        test_cm, \n        title=\"Test Verisi Sonuçları\"\n    )\n\n    test_report_dict = classification_report(\n        y_test, \n        y_test_pred, \n        target_names=target_names, \n        output_dict=True, \n        zero_division=0\n    )\n    test_report_df = pd.DataFrame(test_report_dict).transpose()\n    print(\"\\nSınıf bazlı metrikler (TEST):\")\n    print(test_report_df)\n\n    print(\"\\n\" + \"=\"*80 + \"\\n\")\n\n    return {\n        'train_report': train_report_df,\n        'test_report': test_report_df\n    }\n\n\nfinal_results = {}  \n\n\ndef run_detailed_experiments(clf, preprocessed_data, class_names, classifier_name):\n\n    global final_results  \n\n    best_score = 0.0\n    best_combination = None\n\n    for (vec_name, resample_name), data in preprocessed_data.items():\n        X_resampled = data['X_train']\n        X_test_vec  = data['X_test']\n        y_resampled = data['y_train']\n        y_test_curr = data['y_test']\n\n        _ = extended_classification_report(\n            clf=clf,\n            X_train=X_resampled,\n            y_train=y_resampled,\n            X_test=X_test_vec,\n            y_test=y_test_curr,\n            target_names=class_names,\n            vec_name=vec_name,\n            resample_name=resample_name,\n            classifier_name=classifier_name\n        )\n\n        y_pred_test = clf.predict(X_test_vec)\n        acc = accuracy_score(y_test_curr, y_pred_test)\n        prec_w = precision_score(y_test_curr, y_pred_test, average='weighted', zero_division=0)\n        rec_w  = recall_score(y_test_curr, y_pred_test, average='weighted', zero_division=0)\n        f1_w   = f1_score(y_test_curr, y_pred_test, average='weighted', zero_division=0)\n\n        # prec_m = precision_score(y_test_curr, y_pred_test, average=\"macro\", zero_division=0)\n        # rec_m  = recall_score(y_test_curr, y_pred_test, average=\"macro\", zero_division=0)\n        # f1_m   = f1_score(y_test_curr, y_pred_test, average=\"macro\", zero_division=0)\n\n        if acc > best_score:\n            best_score = acc\n            best_combination = {\n                'vectorizer': vec_name,\n                'resampling': resample_name,\n                'classifier': classifier_name,\n                'accuracy': acc\n            }\n\n        combo_key = f\"{classifier_name} + {vec_name} + {resample_name}\"\n        final_results[combo_key] = {\n            \"Accuracy\": acc,\n            \"Precision(Weighted)\": prec_w,\n            \"Recall(Weighted)\":    rec_w,\n            \"F1-Score(Weighted)\":  f1_w\n            # \"Precision(Macro)\": prec_m, vb. isterseniz\n        }\n\n    if best_combination:\n        print(f\"=== {classifier_name} İçin En İyi Kombinasyon ===\")\n        print(f\"Vektörleştirici : {best_combination['vectorizer']}\")\n        print(f\"Yeniden Örnekleme : {best_combination['resampling']}\")\n        print(f\"Model : {best_combination['classifier']}\")\n        print(f\"Test Accuracy : {best_combination['accuracy']:.4f}\")\n    else:\n        print(f\"{classifier_name} için geçerli bir kombinasyon bulunamadı.\")\n\n    return best_combination\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T09:17:50.744986Z","iopub.execute_input":"2024-12-22T09:17:50.745389Z","iopub.status.idle":"2024-12-22T09:17:50.759102Z","shell.execute_reply.started":"2024-12-22T09:17:50.745363Z","shell.execute_reply":"2024-12-22T09:17:50.758074Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n# Logistic Regression\nlog_reg = LogisticRegression(\n    penalty='l2',\n    C=1.0,\n    solver='lbfgs',\n    max_iter=500,\n    random_state=42\n)\n\n\n\nprint(\"\\n\\n=== LOGISTIC REGRESSION ===\")\nbest_combination_lr = run_detailed_experiments(\n    clf=log_reg,\n    preprocessed_data=preprocessed_data,\n    class_names=class_names,\n    classifier_name=\"Logistic Regression\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T09:18:14.211990Z","iopub.execute_input":"2024-12-22T09:18:14.212300Z","iopub.status.idle":"2024-12-22T09:20:37.622136Z","shell.execute_reply.started":"2024-12-22T09:18:14.212277Z","shell.execute_reply":"2024-12-22T09:20:37.621251Z"}},"outputs":[{"name":"stdout","text":"\n\n=== LOGISTIC REGRESSION ===\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTE, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9990\n  Precision (Ağ.): 0.9990\n  Recall (Ağ.)   : 0.9990\n  F1-Skor (Ağ.)  : 0.9990\n\nKarışıklık Matrisi:\n  6177 18 0\n  1 6194 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999838  0.997094  0.998464   6195.000000\nnefret         0.997102  0.999839  0.998469   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.998978  0.998978  0.998978      0.998978\nmacro avg      0.998980  0.998978  0.998978  18585.000000\nweighted avg   0.998980  0.998978  0.998978  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8298\n  Precision (Ağ.): 0.8231\n  Recall (Ağ.)   : 0.8298\n  F1-Skor (Ağ.)  : 0.8256\n\nKarışıklık Matrisi:\n  1391 131 5\n  188 297 5\n  7 12 9\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.877049  0.910936  0.893672  1527.000000\nnefret         0.675000  0.606122  0.638710   490.000000\nsaldırgan      0.473684  0.321429  0.382979    28.000000\naccuracy       0.829829  0.829829  0.829829     0.829829\nmacro avg      0.675244  0.612829  0.638453  2045.000000\nweighted avg   0.823114  0.829829  0.825588  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5628\n  Precision (Ağ.): 0.7274\n  Recall (Ağ.)   : 0.5628\n  F1-Skor (Ağ.)  : 0.6147\n\nKarışıklık Matrisi:\n  862 464 201\n  142 269 79\n  3 5 20\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.856008  0.564506  0.680347  1527.000000\nnefret         0.364499  0.548980  0.438111   490.000000\nsaldırgan      0.066667  0.714286  0.121951    28.000000\naccuracy       0.562836  0.562836  0.562836     0.562836\nmacro avg      0.429058  0.609257  0.413470  2045.000000\nweighted avg   0.727430  0.562836  0.614660  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTETomek, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9990\n  Precision (Ağ.): 0.9990\n  Recall (Ağ.)   : 0.9990\n  F1-Skor (Ağ.)  : 0.9990\n\nKarışıklık Matrisi:\n  6177 18 0\n  1 6194 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999838  0.997094  0.998464   6195.000000\nnefret         0.997102  0.999839  0.998469   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.998978  0.998978  0.998978      0.998978\nmacro avg      0.998980  0.998978  0.998978  18585.000000\nweighted avg   0.998980  0.998978  0.998978  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8298\n  Precision (Ağ.): 0.8231\n  Recall (Ağ.)   : 0.8298\n  F1-Skor (Ağ.)  : 0.8256\n\nKarışıklık Matrisi:\n  1391 131 5\n  188 297 5\n  7 12 9\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.877049  0.910936  0.893672  1527.000000\nnefret         0.675000  0.606122  0.638710   490.000000\nsaldırgan      0.473684  0.321429  0.382979    28.000000\naccuracy       0.829829  0.829829  0.829829     0.829829\nmacro avg      0.675244  0.612829  0.638453  2045.000000\nweighted avg   0.823114  0.829829  0.825588  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: None, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9976\n  Precision (Ağ.): 0.9976\n  Recall (Ağ.)   : 0.9976\n  F1-Skor (Ağ.)  : 0.9976\n\nKarışıklık Matrisi:\n  6194 1 0\n  17 1829 0\n  1 1 136\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.997102  0.999839  0.998469  6195.000000\nnefret         0.998908  0.990791  0.994833  1846.000000\nsaldırgan      1.000000  0.985507  0.992701   138.000000\naccuracy       0.997555  0.997555  0.997555     0.997555\nmacro avg      0.998670  0.992046  0.995334  8179.000000\nweighted avg   0.997559  0.997555  0.997551  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8391\n  Precision (Ağ.): 0.8269\n  Recall (Ağ.)   : 0.8391\n  F1-Skor (Ağ.)  : 0.8208\n\nKarışıklık Matrisi:\n  1473 53 1\n  247 242 1\n  13 14 1\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score     support\nhiçbiri        0.849971  0.964637  0.903681  1527.00000\nnefret         0.783172  0.493878  0.605757   490.00000\nsaldırgan      0.333333  0.035714  0.064516    28.00000\naccuracy       0.839120  0.839120  0.839120     0.83912\nmacro avg      0.655492  0.498076  0.524651  2045.00000\nweighted avg   0.826892  0.839120  0.820806  2045.00000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTE, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9992\n  Precision (Ağ.): 0.9992\n  Recall (Ağ.)   : 0.9992\n  F1-Skor (Ağ.)  : 0.9992\n\nKarışıklık Matrisi:\n  6182 13 0\n  1 6194 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999838  0.997902  0.998869   6195.000000\nnefret         0.997906  0.999839  0.998871   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999247  0.999247  0.999247      0.999247\nmacro avg      0.999248  0.999247  0.999247  18585.000000\nweighted avg   0.999248  0.999247  0.999247  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8274\n  Precision (Ağ.): 0.8157\n  Recall (Ağ.)   : 0.8274\n  F1-Skor (Ağ.)  : 0.8179\n\nKarışıklık Matrisi:\n  1414 109 4\n  215 274 1\n  12 12 4\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.861670  0.925999  0.892677  1527.000000\nnefret         0.693671  0.559184  0.619209   490.000000\nsaldırgan      0.444444  0.142857  0.216216    28.000000\naccuracy       0.827384  0.827384  0.827384     0.827384\nmacro avg      0.666595  0.542680  0.576034  2045.000000\nweighted avg   0.815703  0.827384  0.817889  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5555\n  Precision (Ağ.): 0.7191\n  Recall (Ağ.)   : 0.5555\n  F1-Skor (Ağ.)  : 0.6062\n\nKarışıklık Matrisi:\n  846 482 199\n  148 273 69\n  5 6 17\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.846847  0.554028  0.669834  1527.000000\nnefret         0.358739  0.557143  0.436451   490.000000\nsaldırgan      0.059649  0.607143  0.108626    28.000000\naccuracy       0.555501  0.555501  0.555501     0.555501\nmacro avg      0.421745  0.572771  0.404970  2045.000000\nweighted avg   0.719114  0.555501  0.606229  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTETomek, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9992\n  Precision (Ağ.): 0.9992\n  Recall (Ağ.)   : 0.9992\n  F1-Skor (Ağ.)  : 0.9992\n\nKarışıklık Matrisi:\n  6182 13 0\n  1 6194 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999838  0.997902  0.998869   6195.000000\nnefret         0.997906  0.999839  0.998871   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999247  0.999247  0.999247      0.999247\nmacro avg      0.999248  0.999247  0.999247  18585.000000\nweighted avg   0.999248  0.999247  0.999247  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8274\n  Precision (Ağ.): 0.8157\n  Recall (Ağ.)   : 0.8274\n  F1-Skor (Ağ.)  : 0.8179\n\nKarışıklık Matrisi:\n  1414 109 4\n  215 274 1\n  12 12 4\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.861670  0.925999  0.892677  1527.000000\nnefret         0.693671  0.559184  0.619209   490.000000\nsaldırgan      0.444444  0.142857  0.216216    28.000000\naccuracy       0.827384  0.827384  0.827384     0.827384\nmacro avg      0.666595  0.542680  0.576034  2045.000000\nweighted avg   0.815703  0.827384  0.817889  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: None, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9996\n  Precision (Ağ.): 0.9996\n  Recall (Ağ.)   : 0.9996\n  F1-Skor (Ağ.)  : 0.9996\n\nKarışıklık Matrisi:\n  6194 1 0\n  2 1844 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.999677  0.999839  0.999758  6195.000000\nnefret         0.999458  0.998917  0.999187  1846.000000\nsaldırgan      1.000000  1.000000  1.000000   138.000000\naccuracy       0.999633  0.999633  0.999633     0.999633\nmacro avg      0.999712  0.999585  0.999648  8179.000000\nweighted avg   0.999633  0.999633  0.999633  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8284\n  Precision (Ağ.): 0.8215\n  Recall (Ağ.)   : 0.8284\n  F1-Skor (Ağ.)  : 0.8026\n\nKarışıklık Matrisi:\n  1490 37 0\n  286 203 1\n  16 11 1\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.831473  0.975769  0.897861  1527.000000\nnefret         0.808765  0.414286  0.547908   490.000000\nsaldırgan      0.500000  0.035714  0.066667    28.000000\naccuracy       0.828362  0.828362  0.828362     0.828362\nmacro avg      0.713413  0.475256  0.504145  2045.000000\nweighted avg   0.821494  0.828362  0.802628  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTE, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9800\n  Precision (Ağ.): 0.9801\n  Recall (Ağ.)   : 0.9800\n  F1-Skor (Ağ.)  : 0.9800\n\nKarışıklık Matrisi:\n  6078 117 0\n  255 5940 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.959735  0.981114  0.970307   6195.000000\nnefret         0.980684  0.958838  0.969638   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.979984  0.979984  0.979984      0.979984\nmacro avg      0.980139  0.979984  0.979981  18585.000000\nweighted avg   0.980139  0.979984  0.979981  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8289\n  Precision (Ağ.): 0.8240\n  Recall (Ağ.)   : 0.8289\n  F1-Skor (Ağ.)  : 0.8259\n\nKarışıklık Matrisi:\n  1371 151 5\n  167 320 3\n  10 14 4\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.885659  0.897839  0.891707  1527.000000\nnefret         0.659794  0.653061  0.656410   490.000000\nsaldırgan      0.333333  0.142857  0.200000    28.000000\naccuracy       0.828851  0.828851  0.828851     0.828851\nmacro avg      0.626262  0.564586  0.582706  2045.000000\nweighted avg   0.823977  0.828851  0.825857  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9976\n  Precision (Ağ.): 0.9976\n  Recall (Ağ.)   : 0.9976\n  F1-Skor (Ağ.)  : 0.9976\n\nKarışıklık Matrisi:\n  138 0 0\n  0 137 1\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        1.000000  1.000000  1.000000  138.000000\nnefret         1.000000  0.992754  0.996364  138.000000\nsaldırgan      0.992806  1.000000  0.996390  138.000000\naccuracy       0.997585  0.997585  0.997585    0.997585\nmacro avg      0.997602  0.997585  0.997585  414.000000\nweighted avg   0.997602  0.997585  0.997585  414.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5765\n  Precision (Ağ.): 0.7399\n  Recall (Ağ.)   : 0.5765\n  F1-Skor (Ağ.)  : 0.6305\n\nKarışıklık Matrisi:\n  892 424 211\n  135 267 88\n  2 6 20\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.866861  0.584152  0.697966  1527.000000\nnefret         0.383070  0.544898  0.449874   490.000000\nsaldırgan      0.062696  0.714286  0.115274    28.000000\naccuracy       0.576528  0.576528  0.576528     0.576528\nmacro avg      0.437542  0.614445  0.421038  2045.000000\nweighted avg   0.739930  0.576528  0.630542  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTETomek, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9800\n  Precision (Ağ.): 0.9801\n  Recall (Ağ.)   : 0.9800\n  F1-Skor (Ağ.)  : 0.9800\n\nKarışıklık Matrisi:\n  6078 117 0\n  255 5940 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.959735  0.981114  0.970307   6195.000000\nnefret         0.980684  0.958838  0.969638   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.979984  0.979984  0.979984      0.979984\nmacro avg      0.980139  0.979984  0.979981  18585.000000\nweighted avg   0.980139  0.979984  0.979981  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8289\n  Precision (Ağ.): 0.8240\n  Recall (Ağ.)   : 0.8289\n  F1-Skor (Ağ.)  : 0.8259\n\nKarışıklık Matrisi:\n  1371 151 5\n  167 320 3\n  10 14 4\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.885659  0.897839  0.891707  1527.000000\nnefret         0.659794  0.653061  0.656410   490.000000\nsaldırgan      0.333333  0.142857  0.200000    28.000000\naccuracy       0.828851  0.828851  0.828851     0.828851\nmacro avg      0.626262  0.564586  0.582706  2045.000000\nweighted avg   0.823977  0.828851  0.825857  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: None, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8957\n  Precision (Ağ.): 0.8885\n  Recall (Ağ.)   : 0.8957\n  F1-Skor (Ağ.)  : 0.8803\n\nKarışıklık Matrisi:\n  6186 9 0\n  706 1140 0\n  118 20 0\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.882454  0.998547  0.936918  6195.000000\nnefret         0.975192  0.617551  0.756219  1846.000000\nsaldırgan      0.000000  0.000000  0.000000   138.000000\naccuracy       0.895709  0.895709  0.895709     0.895709\nmacro avg      0.619215  0.538700  0.564379  8179.000000\nweighted avg   0.888496  0.895709  0.880326  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8240\n  Precision (Ağ.): 0.8151\n  Recall (Ağ.)   : 0.8240\n  F1-Skor (Ağ.)  : 0.7929\n\nKarışıklık Matrisi:\n  1501 26 0\n  306 184 0\n  19 9 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.822015  0.982973  0.895318  1527.000000\nnefret         0.840183  0.375510  0.519041   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.823961  0.823961  0.823961     0.823961\nmacro avg      0.554066  0.452828  0.471453  2045.000000\nweighted avg   0.815113  0.823961  0.792900  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTE, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9872\n  Precision (Ağ.): 0.9875\n  Recall (Ağ.)   : 0.9872\n  F1-Skor (Ağ.)  : 0.9872\n\nKarışıklık Matrisi:\n  6176 19 0\n  219 5976 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.965754  0.996933  0.981096   6195.000000\nnefret         0.996831  0.964649  0.980476   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.987194  0.987194  0.987194      0.987194\nmacro avg      0.987528  0.987194  0.987191  18585.000000\nweighted avg   0.987528  0.987194  0.987191  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8308\n  Precision (Ağ.): 0.8213\n  Recall (Ağ.)   : 0.8308\n  F1-Skor (Ağ.)  : 0.8249\n\nKarışıklık Matrisi:\n  1394 127 6\n  186 302 2\n  11 14 3\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.876179  0.912901  0.894163  1527.000000\nnefret         0.681716  0.616327  0.647374   490.000000\nsaldırgan      0.272727  0.107143  0.153846    28.000000\naccuracy       0.830807  0.830807  0.830807     0.830807\nmacro avg      0.610207  0.545457  0.565128  2045.000000\nweighted avg   0.821321  0.830807  0.824894  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5335\n  Precision (Ağ.): 0.7469\n  Recall (Ağ.)   : 0.5335\n  F1-Skor (Ağ.)  : 0.5962\n\nKarışıklık Matrisi:\n  787 457 283\n  108 284 98\n  2 6 20\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.877369  0.515390  0.649340  1527.000000\nnefret         0.380187  0.579592  0.459175   490.000000\nsaldırgan      0.049875  0.714286  0.093240    28.000000\naccuracy       0.533496  0.533496  0.533496     0.533496\nmacro avg      0.435811  0.603089  0.400585  2045.000000\nweighted avg   0.746910  0.533496  0.596161  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTETomek, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9872\n  Precision (Ağ.): 0.9875\n  Recall (Ağ.)   : 0.9872\n  F1-Skor (Ağ.)  : 0.9872\n\nKarışıklık Matrisi:\n  6176 19 0\n  219 5976 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.965754  0.996933  0.981096   6195.000000\nnefret         0.996831  0.964649  0.980476   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.987194  0.987194  0.987194      0.987194\nmacro avg      0.987528  0.987194  0.987191  18585.000000\nweighted avg   0.987528  0.987194  0.987191  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8308\n  Precision (Ağ.): 0.8213\n  Recall (Ağ.)   : 0.8308\n  F1-Skor (Ağ.)  : 0.8249\n\nKarışıklık Matrisi:\n  1394 127 6\n  186 302 2\n  11 14 3\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.876179  0.912901  0.894163  1527.000000\nnefret         0.681716  0.616327  0.647374   490.000000\nsaldırgan      0.272727  0.107143  0.153846    28.000000\naccuracy       0.830807  0.830807  0.830807     0.830807\nmacro avg      0.610207  0.545457  0.565128  2045.000000\nweighted avg   0.821321  0.830807  0.824894  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: None, Model: Logistic Regression\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8906\n  Precision (Ağ.): 0.8862\n  Recall (Ağ.)   : 0.8906\n  F1-Skor (Ağ.)  : 0.8740\n\nKarışıklık Matrisi:\n  6191 4 0\n  753 1093 0\n  131 7 0\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.875053  0.999354  0.933082  6195.000000\nnefret         0.990036  0.592091  0.741017  1846.000000\nsaldırgan      0.000000  0.000000  0.000000   138.000000\naccuracy       0.890573  0.890573  0.890573     0.890573\nmacro avg      0.621696  0.530482  0.558033  8179.000000\nweighted avg   0.886240  0.890573  0.873990  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8244\n  Precision (Ağ.): 0.8167\n  Recall (Ağ.)   : 0.8244\n  F1-Skor (Ağ.)  : 0.7933\n\nKarışıklık Matrisi:\n  1502 25 0\n  306 184 0\n  20 8 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score     support\nhiçbiri        0.821663  0.983628  0.895380  1527.00000\nnefret         0.847926  0.375510  0.520509   490.00000\nsaldırgan      0.000000  0.000000  0.000000    28.00000\naccuracy       0.824450  0.824450  0.824450     0.82445\nmacro avg      0.556530  0.453046  0.471963  2045.00000\nweighted avg   0.816706  0.824450  0.793298  2045.00000\n\n================================================================================\n\n=== Logistic Regression İçin En İyi Kombinasyon ===\nVektörleştirici : Count Unigram\nYeniden Örnekleme : None\nModel : Logistic Regression\nTest Accuracy : 0.8391\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Random Forest\nrf = RandomForestClassifier(\n    n_estimators=100,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    max_features='sqrt',\n    bootstrap=True,\n    random_state=42\n)\nprint(\"\\n\\n=== RANDOM FOREST ===\")\nbest_combination_rf = run_detailed_experiments(\n    clf=rf,\n    preprocessed_data=preprocessed_data,\n    class_names=class_names,\n    classifier_name=\"Random Forest\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T22:26:23.608623Z","iopub.execute_input":"2024-12-21T22:26:23.608941Z","iopub.status.idle":"2024-12-21T22:32:00.671346Z","shell.execute_reply.started":"2024-12-21T22:26:23.608910Z","shell.execute_reply":"2024-12-21T22:32:00.670625Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n\n=== RANDOM FOREST ===\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTE, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9999\n  Precision (Ağ.): 0.9999\n  Recall (Ağ.)   : 0.9999\n  F1-Skor (Ağ.)  : 0.9999\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 6193 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999677  1.000000  0.999839   6195.000000\nnefret         1.000000  0.999677  0.999839   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999892  0.999892  0.999892      0.999892\nmacro avg      0.999892  0.999892  0.999892  18585.000000\nweighted avg   0.999892  0.999892  0.999892  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7966\n  Precision (Ağ.): 0.8009\n  Recall (Ağ.)   : 0.7966\n  F1-Skor (Ağ.)  : 0.7448\n\nKarışıklık Matrisi:\n  1517 10 0\n  377 112 1\n  22 6 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.791754  0.993451  0.881208  1527.000000\nnefret         0.875000  0.228571  0.362460   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.796577  0.796577  0.796577     0.796577\nmacro avg      0.555585  0.407341  0.414556  2045.000000\nweighted avg   0.800860  0.796577  0.744846  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5174\n  Precision (Ağ.): 0.7211\n  Recall (Ağ.)   : 0.5174\n  F1-Skor (Ağ.)  : 0.5527\n\nKarışıklık Matrisi:\n  697 743 87\n  107 346 37\n  4 9 15\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.862624  0.456451  0.597002  1527.000000\nnefret         0.315118  0.706122  0.435768   490.000000\nsaldırgan      0.107914  0.535714  0.179641    28.000000\naccuracy       0.517359  0.517359  0.517359     0.517359\nmacro avg      0.428552  0.566096  0.404137  2045.000000\nweighted avg   0.721103  0.517359  0.552655  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTETomek, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9999\n  Precision (Ağ.): 0.9999\n  Recall (Ağ.)   : 0.9999\n  F1-Skor (Ağ.)  : 0.9999\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 6193 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999677  1.000000  0.999839   6195.000000\nnefret         1.000000  0.999677  0.999839   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999892  0.999892  0.999892      0.999892\nmacro avg      0.999892  0.999892  0.999892  18585.000000\nweighted avg   0.999892  0.999892  0.999892  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7966\n  Precision (Ağ.): 0.8009\n  Recall (Ağ.)   : 0.7966\n  F1-Skor (Ağ.)  : 0.7448\n\nKarışıklık Matrisi:\n  1517 10 0\n  377 112 1\n  22 6 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.791754  0.993451  0.881208  1527.000000\nnefret         0.875000  0.228571  0.362460   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.796577  0.796577  0.796577     0.796577\nmacro avg      0.555585  0.407341  0.414556  2045.000000\nweighted avg   0.800860  0.796577  0.744846  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: None, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 1844 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.999677  1.000000  0.999839  6195.000000\nnefret         1.000000  0.998917  0.999458  1846.000000\nsaldırgan      1.000000  1.000000  1.000000   138.000000\naccuracy       0.999755  0.999755  0.999755     0.999755\nmacro avg      0.999892  0.999639  0.999766  8179.000000\nweighted avg   0.999756  0.999755  0.999755  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8015\n  Precision (Ağ.): 0.8167\n  Recall (Ağ.)   : 0.8015\n  F1-Skor (Ağ.)  : 0.7529\n\nKarışıklık Matrisi:\n  1518 9 0\n  370 120 0\n  18 9 1\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.796432  0.994106  0.884358  1527.000000\nnefret         0.869565  0.244898  0.382166   490.000000\nsaldırgan      1.000000  0.035714  0.068966    28.000000\naccuracy       0.801467  0.801467  0.801467     0.801467\nmacro avg      0.888666  0.424906  0.445163  2045.000000\nweighted avg   0.816743  0.801467  0.752864  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTE, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9999\n  Precision (Ağ.): 0.9999\n  Recall (Ağ.)   : 0.9999\n  F1-Skor (Ağ.)  : 0.9999\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 6193 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999677  1.000000  0.999839   6195.000000\nnefret         1.000000  0.999677  0.999839   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999892  0.999892  0.999892      0.999892\nmacro avg      0.999892  0.999892  0.999892  18585.000000\nweighted avg   0.999892  0.999892  0.999892  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7834\n  Precision (Ağ.): 0.8056\n  Recall (Ağ.)   : 0.7834\n  F1-Skor (Ağ.)  : 0.7182\n\nKarışıklık Matrisi:\n  1523 3 1\n  411 79 0\n  26 2 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.777041  0.997380  0.873530  1527.000000\nnefret         0.940476  0.161224  0.275261   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.783374  0.783374  0.783374     0.783374\nmacro avg      0.572506  0.386202  0.382931  2045.000000\nweighted avg   0.805562  0.783374  0.718219  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5868\n  Precision (Ağ.): 0.7186\n  Recall (Ağ.)   : 0.5868\n  F1-Skor (Ağ.)  : 0.6197\n\nKarışıklık Matrisi:\n  872 603 52\n  147 314 29\n  6 8 14\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.850732  0.571054  0.683386  1527.000000\nnefret         0.339459  0.640816  0.443816   490.000000\nsaldırgan      0.147368  0.500000  0.227642    28.000000\naccuracy       0.586797  0.586797  0.586797     0.586797\nmacro avg      0.445853  0.570624  0.451615  2045.000000\nweighted avg   0.718596  0.586797  0.619743  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTETomek, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9999\n  Precision (Ağ.): 0.9999\n  Recall (Ağ.)   : 0.9999\n  F1-Skor (Ağ.)  : 0.9999\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 6193 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999677  1.000000  0.999839   6195.000000\nnefret         1.000000  0.999677  0.999839   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999892  0.999892  0.999892      0.999892\nmacro avg      0.999892  0.999892  0.999892  18585.000000\nweighted avg   0.999892  0.999892  0.999892  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7834\n  Precision (Ağ.): 0.8056\n  Recall (Ağ.)   : 0.7834\n  F1-Skor (Ağ.)  : 0.7182\n\nKarışıklık Matrisi:\n  1523 3 1\n  411 79 0\n  26 2 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.777041  0.997380  0.873530  1527.000000\nnefret         0.940476  0.161224  0.275261   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.783374  0.783374  0.783374     0.783374\nmacro avg      0.572506  0.386202  0.382931  2045.000000\nweighted avg   0.805562  0.783374  0.718219  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: None, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 1844 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.999677  1.000000  0.999839  6195.000000\nnefret         1.000000  0.998917  0.999458  1846.000000\nsaldırgan      1.000000  1.000000  1.000000   138.000000\naccuracy       0.999755  0.999755  0.999755     0.999755\nmacro avg      0.999892  0.999639  0.999766  8179.000000\nweighted avg   0.999756  0.999755  0.999755  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7917\n  Precision (Ağ.): 0.8019\n  Recall (Ağ.)   : 0.7917\n  F1-Skor (Ağ.)  : 0.7339\n\nKarışıklık Matrisi:\n  1522 5 0\n  393 97 0\n  22 6 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.785751  0.996726  0.878753  1527.000000\nnefret         0.898148  0.197959  0.324415   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.791687  0.791687  0.791687     0.791687\nmacro avg      0.561300  0.398228  0.401056  2045.000000\nweighted avg   0.801924  0.791687  0.733897  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTE, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9999\n  Precision (Ağ.): 0.9999\n  Recall (Ağ.)   : 0.9999\n  F1-Skor (Ağ.)  : 0.9999\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 6193 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999677  1.000000  0.999839   6195.000000\nnefret         1.000000  0.999677  0.999839   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999892  0.999892  0.999892      0.999892\nmacro avg      0.999892  0.999892  0.999892  18585.000000\nweighted avg   0.999892  0.999892  0.999892  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8112\n  Precision (Ağ.): 0.8037\n  Recall (Ağ.)   : 0.8112\n  F1-Skor (Ağ.)  : 0.7713\n\nKarışıklık Matrisi:\n  1510 17 0\n  341 149 0\n  15 13 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.809218  0.988867  0.890068  1527.000000\nnefret         0.832402  0.304082  0.445441   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.811247  0.811247  0.811247     0.811247\nmacro avg      0.547207  0.430983  0.445170  2045.000000\nweighted avg   0.803693  0.811247  0.771345  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.4983\n  Precision (Ağ.): 0.7244\n  Recall (Ağ.)   : 0.4983\n  F1-Skor (Ağ.)  : 0.5339\n\nKarışıklık Matrisi:\n  652 763 112\n  96 350 44\n  3 8 17\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.868176  0.426981  0.572432  1527.000000\nnefret         0.312221  0.714286  0.434513   490.000000\nsaldırgan      0.098266  0.607143  0.169154    28.000000\naccuracy       0.498289  0.498289  0.498289     0.498289\nmacro avg      0.426221  0.582803  0.392033  2045.000000\nweighted avg   0.724423  0.498289  0.533864  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTETomek, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9999\n  Precision (Ağ.): 0.9999\n  Recall (Ağ.)   : 0.9999\n  F1-Skor (Ağ.)  : 0.9999\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 6193 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999677  1.000000  0.999839   6195.000000\nnefret         1.000000  0.999677  0.999839   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999892  0.999892  0.999892      0.999892\nmacro avg      0.999892  0.999892  0.999892  18585.000000\nweighted avg   0.999892  0.999892  0.999892  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8112\n  Precision (Ağ.): 0.8037\n  Recall (Ağ.)   : 0.8112\n  F1-Skor (Ağ.)  : 0.7713\n\nKarışıklık Matrisi:\n  1510 17 0\n  341 149 0\n  15 13 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.809218  0.988867  0.890068  1527.000000\nnefret         0.832402  0.304082  0.445441   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.811247  0.811247  0.811247     0.811247\nmacro avg      0.547207  0.430983  0.445170  2045.000000\nweighted avg   0.803693  0.811247  0.771345  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: None, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 1844 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.999677  1.000000  0.999839  6195.000000\nnefret         1.000000  0.998917  0.999458  1846.000000\nsaldırgan      1.000000  1.000000  1.000000   138.000000\naccuracy       0.999755  0.999755  0.999755     0.999755\nmacro avg      0.999892  0.999639  0.999766  8179.000000\nweighted avg   0.999756  0.999755  0.999755  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7990\n  Precision (Ağ.): 0.8106\n  Recall (Ağ.)   : 0.7990\n  F1-Skor (Ağ.)  : 0.7493\n\nKarışıklık Matrisi:\n  1517 10 0\n  374 116 0\n  16 11 1\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.795490  0.993451  0.883518  1527.000000\nnefret         0.846715  0.236735  0.370016   490.000000\nsaldırgan      1.000000  0.035714  0.068966    28.000000\naccuracy       0.799022  0.799022  0.799022     0.799022\nmacro avg      0.880735  0.421967  0.440833  2045.000000\nweighted avg   0.810564  0.799022  0.749325  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTE, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9999\n  Precision (Ağ.): 0.9999\n  Recall (Ağ.)   : 0.9999\n  F1-Skor (Ağ.)  : 0.9999\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 6193 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999677  1.000000  0.999839   6195.000000\nnefret         1.000000  0.999677  0.999839   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999892  0.999892  0.999892      0.999892\nmacro avg      0.999892  0.999892  0.999892  18585.000000\nweighted avg   0.999892  0.999892  0.999892  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7971\n  Precision (Ağ.): 0.8050\n  Recall (Ağ.)   : 0.7971\n  F1-Skor (Ağ.)  : 0.7449\n\nKarışıklık Matrisi:\n  1518 9 0\n  378 112 0\n  24 4 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.790625  0.994106  0.880766  1527.000000\nnefret         0.896000  0.228571  0.364228   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.797066  0.797066  0.797066     0.797066\nmacro avg      0.562208  0.407559  0.414998  2045.000000\nweighted avg   0.805049  0.797066  0.744939  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.6093\n  Precision (Ağ.): 0.6963\n  Recall (Ağ.)   : 0.6093\n  F1-Skor (Ağ.)  : 0.6393\n\nKarışıklık Matrisi:\n  983 478 66\n  206 251 33\n  8 8 12\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.821220  0.643746  0.721733  1527.000000\nnefret         0.340570  0.512245  0.409128   490.000000\nsaldırgan      0.108108  0.428571  0.172662    28.000000\naccuracy       0.609291  0.609291  0.609291     0.609291\nmacro avg      0.423299  0.528187  0.434508  2045.000000\nweighted avg   0.696288  0.609291  0.639312  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTETomek, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9999\n  Precision (Ağ.): 0.9999\n  Recall (Ağ.)   : 0.9999\n  F1-Skor (Ağ.)  : 0.9999\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 6193 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999677  1.000000  0.999839   6195.000000\nnefret         1.000000  0.999677  0.999839   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999892  0.999892  0.999892      0.999892\nmacro avg      0.999892  0.999892  0.999892  18585.000000\nweighted avg   0.999892  0.999892  0.999892  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7971\n  Precision (Ağ.): 0.8050\n  Recall (Ağ.)   : 0.7971\n  F1-Skor (Ağ.)  : 0.7449\n\nKarışıklık Matrisi:\n  1518 9 0\n  378 112 0\n  24 4 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.790625  0.994106  0.880766  1527.000000\nnefret         0.896000  0.228571  0.364228   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.797066  0.797066  0.797066     0.797066\nmacro avg      0.562208  0.407559  0.414998  2045.000000\nweighted avg   0.805049  0.797066  0.744939  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: None, Model: Random Forest\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 1844 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.999677  1.000000  0.999839  6195.000000\nnefret         1.000000  0.998917  0.999458  1846.000000\nsaldırgan      1.000000  1.000000  1.000000   138.000000\naccuracy       0.999755  0.999755  0.999755     0.999755\nmacro avg      0.999892  0.999639  0.999766  8179.000000\nweighted avg   0.999756  0.999755  0.999755  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8020\n  Precision (Ağ.): 0.8042\n  Recall (Ağ.)   : 0.8020\n  F1-Skor (Ağ.)  : 0.7544\n\nKarışıklık Matrisi:\n  1515 12 0\n  365 125 0\n  22 6 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.796530  0.992141  0.883640  1527.000000\nnefret         0.874126  0.255102  0.394945   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.801956  0.801956  0.801956     0.801956\nmacro avg      0.556885  0.415748  0.426195  2045.000000\nweighted avg   0.804217  0.801956  0.754445  2045.000000\n\n================================================================================\n\n=== Random Forest İçin En İyi Kombinasyon ===\nVektörleştirici : TFIDF Unigram\nYeniden Örnekleme : SMOTE\nModel : Random Forest\nTest Accuracy : 0.8112\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\nimport xgboost as xgb\n# XGBoost\nxgb_clf = xgb.XGBClassifier(\n    learning_rate=0.03,\n    n_estimators=100,\n    random_state=42,\n    use_label_encoder=False,   \n    eval_metric='logloss'      \n)\n\nprint(\"\\n\\n=== XGBOOST ===\")\nbest_combination_xgb = run_detailed_experiments(\n    clf=xgb_clf,\n    preprocessed_data=preprocessed_data,\n    class_names=class_names,\n    classifier_name=\"XGBoost\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T22:32:00.673272Z","iopub.execute_input":"2024-12-21T22:32:00.673544Z","iopub.status.idle":"2024-12-21T22:45:54.227683Z","shell.execute_reply.started":"2024-12-21T22:32:00.673522Z","shell.execute_reply":"2024-12-21T22:45:54.226948Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n\n=== XGBOOST ===\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTE, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8750\n  Precision (Ağ.): 0.8931\n  Recall (Ağ.)   : 0.8750\n  F1-Skor (Ağ.)  : 0.8736\n\nKarışıklık Matrisi:\n  6000 193 2\n  1833 4311 51\n  141 103 5951\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.752445  0.968523  0.846919   6195.000000\nnefret         0.935750  0.695884  0.798186   6195.000000\nsaldırgan      0.991173  0.960613  0.975654   6195.000000\naccuracy       0.875007  0.875007  0.875007      0.875007\nmacro avg      0.893123  0.875007  0.873586  18585.000000\nweighted avg   0.893123  0.875007  0.873586  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7702\n  Precision (Ağ.): 0.7419\n  Recall (Ağ.)   : 0.7702\n  F1-Skor (Ağ.)  : 0.7181\n\nKarışıklık Matrisi:\n  1479 48 0\n  398 86 6\n  14 4 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.782126  0.968566  0.865418  1527.000000\nnefret         0.623188  0.175510  0.273885   490.000000\nsaldırgan      0.625000  0.357143  0.454545    28.000000\naccuracy       0.770171  0.770171  0.770171     0.770171\nmacro avg      0.676771  0.500406  0.531283  2045.000000\nweighted avg   0.741892  0.770171  0.718056  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7802\n  Precision (Ağ.): 0.8070\n  Recall (Ağ.)   : 0.7802\n  F1-Skor (Ağ.)  : 0.7823\n\nKarışıklık Matrisi:\n  124 11 3\n  37 97 4\n  26 10 102\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.663102  0.898551  0.763077  138.000000\nnefret         0.822034  0.702899  0.757813  138.000000\nsaldırgan      0.935780  0.739130  0.825911  138.000000\naccuracy       0.780193  0.780193  0.780193    0.780193\nmacro avg      0.806972  0.780193  0.782267  414.000000\nweighted avg   0.806972  0.780193  0.782267  414.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5966\n  Precision (Ağ.): 0.7029\n  Recall (Ağ.)   : 0.5966\n  F1-Skor (Ağ.)  : 0.6342\n\nKarışıklık Matrisi:\n  960 444 123\n  194 244 52\n  7 5 16\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.826873  0.628684  0.714286  1527.000000\nnefret         0.352092  0.497959  0.412511   490.000000\nsaldırgan      0.083770  0.571429  0.146119    28.000000\naccuracy       0.596577  0.596577  0.596577     0.596577\nmacro avg      0.420912  0.566024  0.424305  2045.000000\nweighted avg   0.702937  0.596577  0.634198  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTETomek, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8750\n  Precision (Ağ.): 0.8931\n  Recall (Ağ.)   : 0.8750\n  F1-Skor (Ağ.)  : 0.8736\n\nKarışıklık Matrisi:\n  6000 193 2\n  1833 4311 51\n  141 103 5951\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.752445  0.968523  0.846919   6195.000000\nnefret         0.935750  0.695884  0.798186   6195.000000\nsaldırgan      0.991173  0.960613  0.975654   6195.000000\naccuracy       0.875007  0.875007  0.875007      0.875007\nmacro avg      0.893123  0.875007  0.873586  18585.000000\nweighted avg   0.893123  0.875007  0.873586  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7702\n  Precision (Ağ.): 0.7419\n  Recall (Ağ.)   : 0.7702\n  F1-Skor (Ağ.)  : 0.7181\n\nKarışıklık Matrisi:\n  1479 48 0\n  398 86 6\n  14 4 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.782126  0.968566  0.865418  1527.000000\nnefret         0.623188  0.175510  0.273885   490.000000\nsaldırgan      0.625000  0.357143  0.454545    28.000000\naccuracy       0.770171  0.770171  0.770171     0.770171\nmacro avg      0.676771  0.500406  0.531283  2045.000000\nweighted avg   0.741892  0.770171  0.718056  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: None, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8094\n  Precision (Ağ.): 0.8395\n  Recall (Ağ.)   : 0.8094\n  F1-Skor (Ağ.)  : 0.7594\n\nKarışıklık Matrisi:\n  6187 7 1\n  1439 401 6\n  101 5 32\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.800699  0.998709  0.888809  6195.00000\nnefret         0.970944  0.217226  0.355024  1846.00000\nsaldırgan      0.820513  0.231884  0.361582   138.00000\naccuracy       0.809390  0.809390  0.809390     0.80939\nmacro avg      0.864052  0.482606  0.535138  8179.00000\nweighted avg   0.839458  0.809390  0.759438  8179.00000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7839\n  Precision (Ağ.): 0.8009\n  Recall (Ağ.)   : 0.7839\n  F1-Skor (Ağ.)  : 0.7214\n\nKarışıklık Matrisi:\n  1519 8 0\n  411 74 5\n  15 3 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.780977  0.994761  0.875000  1527.000000\nnefret         0.870588  0.151020  0.257391   490.000000\nsaldırgan      0.666667  0.357143  0.465116    28.000000\naccuracy       0.783863  0.783863  0.783863     0.783863\nmacro avg      0.772744  0.500975  0.532503  2045.000000\nweighted avg   0.800883  0.783863  0.721403  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTE, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8913\n  Precision (Ağ.): 0.9072\n  Recall (Ağ.)   : 0.8913\n  F1-Skor (Ağ.)  : 0.8902\n\nKarışıklık Matrisi:\n  6018 176 1\n  1663 4494 38\n  107 35 6053\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.772727  0.971429  0.860759   6195.00000\nnefret         0.955154  0.725424  0.824587   6195.00000\nsaldırgan      0.993598  0.977078  0.985269   6195.00000\naccuracy       0.891310  0.891310  0.891310      0.89131\nmacro avg      0.907160  0.891310  0.890205  18585.00000\nweighted avg   0.907160  0.891310  0.890205  18585.00000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7643\n  Precision (Ağ.): 0.7295\n  Recall (Ağ.)   : 0.7643\n  F1-Skor (Ağ.)  : 0.7083\n\nKarışıklık Matrisi:\n  1478 49 0\n  409 75 6\n  14 4 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.777486  0.967911  0.862310  1527.000000\nnefret         0.585938  0.153061  0.242718   490.000000\nsaldırgan      0.625000  0.357143  0.454545    28.000000\naccuracy       0.764303  0.764303  0.764303     0.764303\nmacro avg      0.662808  0.492705  0.519858  2045.000000\nweighted avg   0.729501  0.764303  0.708268  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7754\n  Precision (Ağ.): 0.8006\n  Recall (Ağ.)   : 0.7754\n  F1-Skor (Ağ.)  : 0.7773\n\nKarışıklık Matrisi:\n  123 12 3\n  37 97 4\n  25 12 101\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.664865  0.891304  0.761610  138.000000\nnefret         0.801653  0.702899  0.749035  138.000000\nsaldırgan      0.935185  0.731884  0.821138  138.000000\naccuracy       0.775362  0.775362  0.775362    0.775362\nmacro avg      0.800568  0.775362  0.777261  414.000000\nweighted avg   0.800568  0.775362  0.777261  414.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5951\n  Precision (Ağ.): 0.7019\n  Recall (Ağ.)   : 0.5951\n  F1-Skor (Ağ.)  : 0.6325\n\nKarışıklık Matrisi:\n  957 449 121\n  195 243 52\n  6 5 17\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score     support\nhiçbiri        0.826425  0.626719  0.712849  1527.00000\nnefret         0.348637  0.495918  0.409436   490.00000\nsaldırgan      0.089474  0.607143  0.155963    28.00000\naccuracy       0.595110  0.595110  0.595110     0.59511\nmacro avg      0.421512  0.576593  0.426083  2045.00000\nweighted avg   0.701852  0.595110  0.632524  2045.00000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTETomek, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8913\n  Precision (Ağ.): 0.9072\n  Recall (Ağ.)   : 0.8913\n  F1-Skor (Ağ.)  : 0.8902\n\nKarışıklık Matrisi:\n  6018 176 1\n  1663 4494 38\n  107 35 6053\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.772727  0.971429  0.860759   6195.00000\nnefret         0.955154  0.725424  0.824587   6195.00000\nsaldırgan      0.993598  0.977078  0.985269   6195.00000\naccuracy       0.891310  0.891310  0.891310      0.89131\nmacro avg      0.907160  0.891310  0.890205  18585.00000\nweighted avg   0.907160  0.891310  0.890205  18585.00000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7643\n  Precision (Ağ.): 0.7295\n  Recall (Ağ.)   : 0.7643\n  F1-Skor (Ağ.)  : 0.7083\n\nKarışıklık Matrisi:\n  1478 49 0\n  409 75 6\n  14 4 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.777486  0.967911  0.862310  1527.000000\nnefret         0.585938  0.153061  0.242718   490.000000\nsaldırgan      0.625000  0.357143  0.454545    28.000000\naccuracy       0.764303  0.764303  0.764303     0.764303\nmacro avg      0.662808  0.492705  0.519858  2045.000000\nweighted avg   0.729501  0.764303  0.708268  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: None, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8113\n  Precision (Ağ.): 0.8414\n  Recall (Ağ.)   : 0.8113\n  F1-Skor (Ağ.)  : 0.7627\n\nKarışıklık Matrisi:\n  6188 6 1\n  1424 416 6\n  101 5 32\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.802282  0.998870  0.889848  6195.000000\nnefret         0.974239  0.225352  0.366036  1846.000000\nsaldırgan      0.820513  0.231884  0.361582   138.000000\naccuracy       0.811346  0.811346  0.811346     0.811346\nmacro avg      0.865678  0.485369  0.539155  8179.000000\nweighted avg   0.841400  0.811346  0.762710  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7819\n  Precision (Ağ.): 0.7928\n  Recall (Ağ.)   : 0.7819\n  F1-Skor (Ağ.)  : 0.7194\n\nKarışıklık Matrisi:\n  1516 11 0\n  412 73 5\n  15 3 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.780237  0.992796  0.873775  1527.000000\nnefret         0.839080  0.148980  0.253033   490.000000\nsaldırgan      0.666667  0.357143  0.465116    28.000000\naccuracy       0.781907  0.781907  0.781907     0.781907\nmacro avg      0.761995  0.499640  0.530641  2045.000000\nweighted avg   0.792781  0.781907  0.719445  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTE, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8433\n  Precision (Ağ.): 0.8570\n  Recall (Ağ.)   : 0.8433\n  F1-Skor (Ağ.)  : 0.8432\n\nKarışıklık Matrisi:\n  5665 506 24\n  1765 4309 121\n  341 156 5698\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.728992  0.914447  0.811256   6195.000000\nnefret         0.866828  0.695561  0.771807   6195.000000\nsaldırgan      0.975184  0.919774  0.946669   6195.000000\naccuracy       0.843261  0.843261  0.843261      0.843261\nmacro avg      0.857001  0.843261  0.843244  18585.000000\nweighted avg   0.857001  0.843261  0.843244  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7555\n  Precision (Ağ.): 0.7278\n  Recall (Ağ.)   : 0.7555\n  F1-Skor (Ağ.)  : 0.7336\n\nKarışıklık Matrisi:\n  1380 142 5\n  330 153 7\n  11 5 12\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.801859  0.903733  0.849754  1527.000000\nnefret         0.510000  0.312245  0.387342   490.000000\nsaldırgan      0.500000  0.428571  0.461538    28.000000\naccuracy       0.755501  0.755501  0.755501     0.755501\nmacro avg      0.603953  0.548183  0.566211  2045.000000\nweighted avg   0.727794  0.755501  0.733640  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8792\n  Precision (Ağ.): 0.8947\n  Recall (Ağ.)   : 0.8792\n  F1-Skor (Ağ.)  : 0.8802\n\nKarışıklık Matrisi:\n  135 2 1\n  24 112 2\n  15 6 117\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.775862  0.978261  0.865385  138.000000\nnefret         0.933333  0.811594  0.868217  138.000000\nsaldırgan      0.975000  0.847826  0.906977  138.000000\naccuracy       0.879227  0.879227  0.879227    0.879227\nmacro avg      0.894732  0.879227  0.880193  414.000000\nweighted avg   0.894732  0.879227  0.880193  414.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5800\n  Precision (Ağ.): 0.7003\n  Recall (Ağ.)   : 0.5800\n  F1-Skor (Ağ.)  : 0.6214\n\nKarışıklık Matrisi:\n  921 443 163\n  194 247 49\n  5 5 18\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.822321  0.603143  0.695882  1527.000000\nnefret         0.355396  0.504082  0.416878   490.000000\nsaldırgan      0.078261  0.642857  0.139535    28.000000\naccuracy       0.579951  0.579951  0.579951     0.579951\nmacro avg      0.418659  0.583361  0.417432  2045.000000\nweighted avg   0.700254  0.579951  0.621413  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTETomek, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8433\n  Precision (Ağ.): 0.8570\n  Recall (Ağ.)   : 0.8433\n  F1-Skor (Ağ.)  : 0.8432\n\nKarışıklık Matrisi:\n  5665 506 24\n  1765 4309 121\n  341 156 5698\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.728992  0.914447  0.811256   6195.000000\nnefret         0.866828  0.695561  0.771807   6195.000000\nsaldırgan      0.975184  0.919774  0.946669   6195.000000\naccuracy       0.843261  0.843261  0.843261      0.843261\nmacro avg      0.857001  0.843261  0.843244  18585.000000\nweighted avg   0.857001  0.843261  0.843244  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7555\n  Precision (Ağ.): 0.7278\n  Recall (Ağ.)   : 0.7555\n  F1-Skor (Ağ.)  : 0.7336\n\nKarışıklık Matrisi:\n  1380 142 5\n  330 153 7\n  11 5 12\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.801859  0.903733  0.849754  1527.000000\nnefret         0.510000  0.312245  0.387342   490.000000\nsaldırgan      0.500000  0.428571  0.461538    28.000000\naccuracy       0.755501  0.755501  0.755501     0.755501\nmacro avg      0.603953  0.548183  0.566211  2045.000000\nweighted avg   0.727794  0.755501  0.733640  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: None, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8121\n  Precision (Ağ.): 0.8437\n  Recall (Ağ.)   : 0.8121\n  F1-Skor (Ağ.)  : 0.7638\n\nKarışıklık Matrisi:\n  6189 5 1\n  1426 418 2\n  98 5 35\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.802412  0.999031  0.889991  6195.00000\nnefret         0.976636  0.226436  0.367634  1846.00000\nsaldırgan      0.921053  0.253623  0.397727   138.00000\naccuracy       0.812080  0.812080  0.812080     0.81208\nmacro avg      0.900033  0.493030  0.551784  8179.00000\nweighted avg   0.843736  0.812080  0.763790  8179.00000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7839\n  Precision (Ağ.): 0.7875\n  Recall (Ağ.)   : 0.7839\n  F1-Skor (Ağ.)  : 0.7249\n\nKarışıklık Matrisi:\n  1513 14 0\n  404 80 6\n  13 5 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.783938  0.990832  0.875325  1527.000000\nnefret         0.808081  0.163265  0.271647   490.000000\nsaldırgan      0.625000  0.357143  0.454545    28.000000\naccuracy       0.783863  0.783863  0.783863     0.783863\nmacro avg      0.739006  0.503747  0.533839  2045.000000\nweighted avg   0.787547  0.783863  0.724917  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTE, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8378\n  Precision (Ağ.): 0.8521\n  Recall (Ağ.)   : 0.8378\n  F1-Skor (Ağ.)  : 0.8379\n\nKarışıklık Matrisi:\n  5631 542 22\n  1794 4265 136\n  379 141 5675\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.721553  0.908959  0.804486   6195.000000\nnefret         0.861964  0.688458  0.765503   6195.000000\nsaldırgan      0.972913  0.916061  0.943632   6195.000000\naccuracy       0.837826  0.837826  0.837826      0.837826\nmacro avg      0.852143  0.837826  0.837874  18585.000000\nweighted avg   0.852143  0.837826  0.837874  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7482\n  Precision (Ağ.): 0.7202\n  Recall (Ağ.)   : 0.7482\n  F1-Skor (Ağ.)  : 0.7275\n\nKarışıklık Matrisi:\n  1367 156 4\n  329 153 8\n  13 5 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.799883  0.895219  0.844870  1527.000000\nnefret         0.487261  0.312245  0.380597   490.000000\nsaldırgan      0.454545  0.357143  0.400000    28.000000\naccuracy       0.748166  0.748166  0.748166     0.748166\nmacro avg      0.580563  0.521536  0.541822  2045.000000\nweighted avg   0.720248  0.748166  0.727535  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8768\n  Precision (Ağ.): 0.8943\n  Recall (Ağ.)   : 0.8768\n  F1-Skor (Ağ.)  : 0.8784\n\nKarışıklık Matrisi:\n  134 3 1\n  23 114 1\n  18 5 115\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.765714  0.971014  0.856230  138.000000\nnefret         0.934426  0.826087  0.876923  138.000000\nsaldırgan      0.982906  0.833333  0.901961  138.000000\naccuracy       0.876812  0.876812  0.876812    0.876812\nmacro avg      0.894349  0.876812  0.878371  414.000000\nweighted avg   0.894349  0.876812  0.878371  414.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5619\n  Precision (Ağ.): 0.6792\n  Recall (Ağ.)   : 0.5619\n  F1-Skor (Ağ.)  : 0.6034\n\nKarışıklık Matrisi:\n  911 469 147\n  214 222 54\n  5 7 16\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.806195  0.596595  0.685736  1527.000000\nnefret         0.318052  0.453061  0.373737   490.000000\nsaldırgan      0.073733  0.571429  0.130612    28.000000\naccuracy       0.561858  0.561858  0.561858     0.561858\nmacro avg      0.399326  0.540361  0.396695  2045.000000\nweighted avg   0.679202  0.561858  0.603378  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTETomek, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8378\n  Precision (Ağ.): 0.8521\n  Recall (Ağ.)   : 0.8378\n  F1-Skor (Ağ.)  : 0.8379\n\nKarışıklık Matrisi:\n  5631 542 22\n  1794 4265 136\n  379 141 5675\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.721553  0.908959  0.804486   6195.000000\nnefret         0.861964  0.688458  0.765503   6195.000000\nsaldırgan      0.972913  0.916061  0.943632   6195.000000\naccuracy       0.837826  0.837826  0.837826      0.837826\nmacro avg      0.852143  0.837826  0.837874  18585.000000\nweighted avg   0.852143  0.837826  0.837874  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7482\n  Precision (Ağ.): 0.7202\n  Recall (Ağ.)   : 0.7482\n  F1-Skor (Ağ.)  : 0.7275\n\nKarışıklık Matrisi:\n  1367 156 4\n  329 153 8\n  13 5 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.799883  0.895219  0.844870  1527.000000\nnefret         0.487261  0.312245  0.380597   490.000000\nsaldırgan      0.454545  0.357143  0.400000    28.000000\naccuracy       0.748166  0.748166  0.748166     0.748166\nmacro avg      0.580563  0.521536  0.541822  2045.000000\nweighted avg   0.720248  0.748166  0.727535  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: None, Model: XGBoost\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8132\n  Precision (Ağ.): 0.8464\n  Recall (Ağ.)   : 0.8132\n  F1-Skor (Ağ.)  : 0.7652\n\nKarışıklık Matrisi:\n  6193 1 1\n  1419 424 3\n  100 4 34\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.803034  0.999677  0.890631  6195.00000\nnefret         0.988345  0.229686  0.372747  1846.00000\nsaldırgan      0.894737  0.246377  0.386364   138.00000\naccuracy       0.813180  0.813180  0.813180     0.81318\nmacro avg      0.895372  0.491913  0.549914  8179.00000\nweighted avg   0.846406  0.813180  0.765236  8179.00000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7839\n  Precision (Ağ.): 0.7860\n  Recall (Ağ.)   : 0.7839\n  F1-Skor (Ağ.)  : 0.7271\n\nKarışıklık Matrisi:\n  1509 18 0\n  399 85 6\n  16 3 9\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.784304  0.988212  0.874529  1527.000000\nnefret         0.801887  0.173469  0.285235   490.000000\nsaldırgan      0.600000  0.321429  0.418605    28.000000\naccuracy       0.783863  0.783863  0.783863     0.783863\nmacro avg      0.728730  0.494370  0.526123  2045.000000\nweighted avg   0.785993  0.783863  0.727087  2045.000000\n\n================================================================================\n\n=== XGBoost İçin En İyi Kombinasyon ===\nVektörleştirici : Count Unigram\nYeniden Örnekleme : None\nModel : XGBoost\nTest Accuracy : 0.7839\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import lightgbm as lgb\n\n\n# LightGBM\nligtgbm = lgb.LGBMClassifier(\n    learning_rate=0.03,\n    n_estimators=100,\n    random_state=42\n)\nprint(\"\\n\\n=== LIGHTGBM ===\")\nbest_combination_ligtgbm = run_detailed_experiments(\n    clf=ligtgbm,\n    preprocessed_data=preprocessed_data,\n    class_names=class_names,\n    classifier_name=\"LightGBM\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T22:45:54.228565Z","iopub.execute_input":"2024-12-21T22:45:54.228822Z","iopub.status.idle":"2024-12-21T22:47:39.486536Z","shell.execute_reply.started":"2024-12-21T22:45:54.228798Z","shell.execute_reply":"2024-12-21T22:47:39.485714Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n\n=== LIGHTGBM ===\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157315 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 69637\n[LightGBM] [Info] Number of data points in the train set: 18585, number of used features: 2956\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTE, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9205\n  Precision (Ağ.): 0.9303\n  Recall (Ağ.)   : 0.9205\n  F1-Skor (Ağ.)  : 0.9200\n\nKarışıklık Matrisi:\n  6084 110 1\n  1238 4918 39\n  72 18 6105\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.822829  0.982082  0.895430   6195.000000\nnefret         0.974633  0.793866  0.875011   6195.000000\nsaldırgan      0.993491  0.985472  0.989465   6195.000000\naccuracy       0.920474  0.920474  0.920474      0.920474\nmacro avg      0.930318  0.920474  0.919969  18585.000000\nweighted avg   0.930318  0.920474  0.919969  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8049\n  Precision (Ağ.): 0.7993\n  Recall (Ağ.)   : 0.8049\n  F1-Skor (Ağ.)  : 0.7693\n\nKarışıklık Matrisi:\n  1493 34 0\n  337 143 10\n  12 6 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score     support\nhiçbiri        0.810532  0.977734  0.886316  1527.00000\nnefret         0.781421  0.291837  0.424963   490.00000\nsaldırgan      0.500000  0.357143  0.416667    28.00000\naccuracy       0.804890  0.804890  0.804890     0.80489\nmacro avg      0.697318  0.542238  0.575982  2045.00000\nweighted avg   0.799305  0.804890  0.769342  2045.00000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001390 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 89\n[LightGBM] [Info] Number of data points in the train set: 414, number of used features: 26\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.6111\n  Precision (Ağ.): 0.6114\n  Recall (Ağ.)   : 0.6111\n  F1-Skor (Ağ.)  : 0.6111\n\nKarışıklık Matrisi:\n  85 23 30\n  30 87 21\n  29 28 81\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.590278  0.615942  0.602837  138.000000\nnefret         0.630435  0.630435  0.630435  138.000000\nsaldırgan      0.613636  0.586957  0.600000  138.000000\naccuracy       0.611111  0.611111  0.611111    0.611111\nmacro avg      0.611450  0.611111  0.611091  414.000000\nweighted avg   0.611450  0.611111  0.611091  414.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.4533\n  Precision (Ağ.): 0.7146\n  Recall (Ağ.)   : 0.4533\n  F1-Skor (Ağ.)  : 0.5275\n\nKarışıklık Matrisi:\n  652 439 436\n  118 260 112\n  8 5 15\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.838046  0.426981  0.565727  1527.000000\nnefret         0.369318  0.530612  0.435511   490.000000\nsaldırgan      0.026643  0.535714  0.050761    28.000000\naccuracy       0.453301  0.453301  0.453301     0.453301\nmacro avg      0.411336  0.497769  0.350666  2045.000000\nweighted avg   0.714625  0.453301  0.527475  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154258 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 69637\n[LightGBM] [Info] Number of data points in the train set: 18585, number of used features: 2956\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTETomek, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9205\n  Precision (Ağ.): 0.9303\n  Recall (Ağ.)   : 0.9205\n  F1-Skor (Ağ.)  : 0.9200\n\nKarışıklık Matrisi:\n  6084 110 1\n  1238 4918 39\n  72 18 6105\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.822829  0.982082  0.895430   6195.000000\nnefret         0.974633  0.793866  0.875011   6195.000000\nsaldırgan      0.993491  0.985472  0.989465   6195.000000\naccuracy       0.920474  0.920474  0.920474      0.920474\nmacro avg      0.930318  0.920474  0.919969  18585.000000\nweighted avg   0.930318  0.920474  0.919969  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8049\n  Precision (Ağ.): 0.7993\n  Recall (Ağ.)   : 0.8049\n  F1-Skor (Ağ.)  : 0.7693\n\nKarışıklık Matrisi:\n  1493 34 0\n  337 143 10\n  12 6 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score     support\nhiçbiri        0.810532  0.977734  0.886316  1527.00000\nnefret         0.781421  0.291837  0.424963   490.00000\nsaldırgan      0.500000  0.357143  0.416667    28.00000\naccuracy       0.804890  0.804890  0.804890     0.80489\nmacro avg      0.697318  0.542238  0.575982  2045.00000\nweighted avg   0.799305  0.804890  0.769342  2045.00000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025537 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2882\n[LightGBM] [Info] Number of data points in the train set: 8179, number of used features: 966\n[LightGBM] [Info] Start training from score -0.277827\n[LightGBM] [Info] Start training from score -1.488549\n[LightGBM] [Info] Start training from score -4.082071\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: None, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8409\n  Precision (Ağ.): 0.8534\n  Recall (Ağ.)   : 0.8409\n  F1-Skor (Ağ.)  : 0.8136\n\nKarışıklık Matrisi:\n  6143 52 0\n  1161 683 2\n  75 11 52\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.832498  0.991606  0.905113  6195.000000\nnefret         0.915550  0.369989  0.527006  1846.000000\nsaldırgan      0.962963  0.376812  0.541667   138.000000\naccuracy       0.840934  0.840934  0.840934     0.840934\nmacro avg      0.903670  0.579469  0.657929  8179.000000\nweighted avg   0.853444  0.840934  0.813642  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8044\n  Precision (Ağ.): 0.8033\n  Recall (Ağ.)   : 0.8044\n  F1-Skor (Ağ.)  : 0.7654\n\nKarışıklık Matrisi:\n  1501 26 0\n  351 135 4\n  12 7 9\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.805258  0.982973  0.885285  1527.000000\nnefret         0.803571  0.275510  0.410334   490.000000\nsaldırgan      0.692308  0.321429  0.439024    28.000000\naccuracy       0.804401  0.804401  0.804401     0.804401\nmacro avg      0.767046  0.526637  0.578214  2045.000000\nweighted avg   0.803307  0.804401  0.765372  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.357827 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 123225\n[LightGBM] [Info] Number of data points in the train set: 18585, number of used features: 5901\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTE, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9224\n  Precision (Ağ.): 0.9338\n  Recall (Ağ.)   : 0.9224\n  F1-Skor (Ağ.)  : 0.9220\n\nKarışıklık Matrisi:\n  6133 61 1\n  1270 4912 13\n  80 18 6097\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.819591  0.989992  0.896769   6195.000000\nnefret         0.984172  0.792897  0.878241   6195.000000\nsaldırgan      0.997709  0.984181  0.990899   6195.000000\naccuracy       0.922357  0.922357  0.922357      0.922357\nmacro avg      0.933824  0.922357  0.921969  18585.000000\nweighted avg   0.933824  0.922357  0.921969  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8054\n  Precision (Ağ.): 0.8075\n  Recall (Ağ.)   : 0.8054\n  F1-Skor (Ağ.)  : 0.7658\n\nKarışıklık Matrisi:\n  1504 23 0\n  350 133 7\n  13 5 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.805570  0.984938  0.886270  1527.000000\nnefret         0.826087  0.271429  0.408602   490.000000\nsaldırgan      0.588235  0.357143  0.444444    28.000000\naccuracy       0.805379  0.805379  0.805379     0.805379\nmacro avg      0.739964  0.537836  0.579772  2045.000000\nweighted avg   0.807511  0.805379  0.765767  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000282 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 89\n[LightGBM] [Info] Number of data points in the train set: 414, number of used features: 26\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.6111\n  Precision (Ağ.): 0.6114\n  Recall (Ağ.)   : 0.6111\n  F1-Skor (Ağ.)  : 0.6111\n\nKarışıklık Matrisi:\n  85 23 30\n  30 87 21\n  29 28 81\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.590278  0.615942  0.602837  138.000000\nnefret         0.630435  0.630435  0.630435  138.000000\nsaldırgan      0.613636  0.586957  0.600000  138.000000\naccuracy       0.611111  0.611111  0.611111    0.611111\nmacro avg      0.611450  0.611111  0.611091  414.000000\nweighted avg   0.611450  0.611111  0.611091  414.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.4533\n  Precision (Ağ.): 0.7146\n  Recall (Ağ.)   : 0.4533\n  F1-Skor (Ağ.)  : 0.5275\n\nKarışıklık Matrisi:\n  652 439 436\n  118 260 112\n  8 5 15\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.838046  0.426981  0.565727  1527.000000\nnefret         0.369318  0.530612  0.435511   490.000000\nsaldırgan      0.026643  0.535714  0.050761    28.000000\naccuracy       0.453301  0.453301  0.453301     0.453301\nmacro avg      0.411336  0.497769  0.350666  2045.000000\nweighted avg   0.714625  0.453301  0.527475  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.350986 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 123225\n[LightGBM] [Info] Number of data points in the train set: 18585, number of used features: 5901\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTETomek, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9224\n  Precision (Ağ.): 0.9338\n  Recall (Ağ.)   : 0.9224\n  F1-Skor (Ağ.)  : 0.9220\n\nKarışıklık Matrisi:\n  6133 61 1\n  1270 4912 13\n  80 18 6097\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.819591  0.989992  0.896769   6195.000000\nnefret         0.984172  0.792897  0.878241   6195.000000\nsaldırgan      0.997709  0.984181  0.990899   6195.000000\naccuracy       0.922357  0.922357  0.922357      0.922357\nmacro avg      0.933824  0.922357  0.921969  18585.000000\nweighted avg   0.933824  0.922357  0.921969  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8054\n  Precision (Ağ.): 0.8075\n  Recall (Ağ.)   : 0.8054\n  F1-Skor (Ağ.)  : 0.7658\n\nKarışıklık Matrisi:\n  1504 23 0\n  350 133 7\n  13 5 10\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.805570  0.984938  0.886270  1527.000000\nnefret         0.826087  0.271429  0.408602   490.000000\nsaldırgan      0.588235  0.357143  0.444444    28.000000\naccuracy       0.805379  0.805379  0.805379     0.805379\nmacro avg      0.739964  0.537836  0.579772  2045.000000\nweighted avg   0.807511  0.805379  0.765767  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046636 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3132\n[LightGBM] [Info] Number of data points in the train set: 8179, number of used features: 1067\n[LightGBM] [Info] Start training from score -0.277827\n[LightGBM] [Info] Start training from score -1.488549\n[LightGBM] [Info] Start training from score -4.082071\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: None, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8414\n  Precision (Ağ.): 0.8542\n  Recall (Ağ.)   : 0.8414\n  F1-Skor (Ağ.)  : 0.8142\n\nKarışıklık Matrisi:\n  6145 50 0\n  1161 683 2\n  73 11 54\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.832769  0.991929  0.905407  6195.000000\nnefret         0.918011  0.369989  0.527413  1846.000000\nsaldırgan      0.964286  0.391304  0.556701   138.000000\naccuracy       0.841423  0.841423  0.841423     0.841423\nmacro avg      0.905022  0.584407  0.663174  8179.000000\nweighted avg   0.854227  0.841423  0.814211  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8039\n  Precision (Ağ.): 0.8017\n  Recall (Ağ.)   : 0.8039\n  F1-Skor (Ağ.)  : 0.7650\n\nKarışıklık Matrisi:\n  1500 27 0\n  351 136 3\n  12 8 8\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.805153  0.982318  0.884956  1527.000000\nnefret         0.795322  0.277551  0.411498   490.000000\nsaldırgan      0.727273  0.285714  0.410256    28.000000\naccuracy       0.803912  0.803912  0.803912     0.803912\nmacro avg      0.775916  0.515195  0.568903  2045.000000\nweighted avg   0.801731  0.803912  0.765011  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173421 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 113120\n[LightGBM] [Info] Number of data points in the train set: 18585, number of used features: 3242\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTE, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9085\n  Precision (Ağ.): 0.9116\n  Recall (Ağ.)   : 0.9085\n  F1-Skor (Ağ.)  : 0.9080\n\nKarışıklık Matrisi:\n  5732 433 30\n  1130 4996 69\n  33 6 6156\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.831327  0.925262  0.875783   6195.000000\nnefret         0.919227  0.806457  0.859157   6195.000000\nsaldırgan      0.984173  0.993705  0.988916   6195.000000\naccuracy       0.908475  0.908475  0.908475      0.908475\nmacro avg      0.911576  0.908475  0.907952  18585.000000\nweighted avg   0.911576  0.908475  0.907952  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8020\n  Precision (Ağ.): 0.7886\n  Recall (Ağ.)   : 0.8020\n  F1-Skor (Ağ.)  : 0.7902\n\nKarışıklık Matrisi:\n  1404 115 8\n  255 225 10\n  9 8 11\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.841727  0.919450  0.878873  1527.000000\nnefret         0.646552  0.459184  0.536993   490.000000\nsaldırgan      0.379310  0.392857  0.385965    28.000000\naccuracy       0.801956  0.801956  0.801956     0.801956\nmacro avg      0.622530  0.590497  0.600610  2045.000000\nweighted avg   0.788630  0.801956  0.790207  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000353 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 407\n[LightGBM] [Info] Number of data points in the train set: 414, number of used features: 26\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7126\n  Precision (Ağ.): 0.7142\n  Recall (Ağ.)   : 0.7126\n  F1-Skor (Ağ.)  : 0.7129\n\nKarışıklık Matrisi:\n  96 19 23\n  26 97 15\n  23 13 102\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score    support\nhiçbiri        0.662069  0.695652  0.678445  138.00000\nnefret         0.751938  0.702899  0.726592  138.00000\nsaldırgan      0.728571  0.739130  0.733813  138.00000\naccuracy       0.712560  0.712560  0.712560    0.71256\nmacro avg      0.714193  0.712560  0.712950  414.00000\nweighted avg   0.714193  0.712560  0.712950  414.00000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.4548\n  Precision (Ağ.): 0.6990\n  Recall (Ağ.)   : 0.4548\n  F1-Skor (Ağ.)  : 0.5283\n\nKarışıklık Matrisi:\n  677 446 404\n  136 239 115\n  8 6 14\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.824604  0.443353  0.576661  1527.000000\nnefret         0.345876  0.487755  0.404742   490.000000\nsaldırgan      0.026266  0.500000  0.049911    28.000000\naccuracy       0.454768  0.454768  0.454768     0.454768\nmacro avg      0.398915  0.477036  0.343771  2045.000000\nweighted avg   0.698966  0.454768  0.528255  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176527 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 113120\n[LightGBM] [Info] Number of data points in the train set: 18585, number of used features: 3242\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTETomek, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9085\n  Precision (Ağ.): 0.9116\n  Recall (Ağ.)   : 0.9085\n  F1-Skor (Ağ.)  : 0.9080\n\nKarışıklık Matrisi:\n  5732 433 30\n  1130 4996 69\n  33 6 6156\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.831327  0.925262  0.875783   6195.000000\nnefret         0.919227  0.806457  0.859157   6195.000000\nsaldırgan      0.984173  0.993705  0.988916   6195.000000\naccuracy       0.908475  0.908475  0.908475      0.908475\nmacro avg      0.911576  0.908475  0.907952  18585.000000\nweighted avg   0.911576  0.908475  0.907952  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8020\n  Precision (Ağ.): 0.7886\n  Recall (Ağ.)   : 0.8020\n  F1-Skor (Ağ.)  : 0.7902\n\nKarışıklık Matrisi:\n  1404 115 8\n  255 225 10\n  9 8 11\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.841727  0.919450  0.878873  1527.000000\nnefret         0.646552  0.459184  0.536993   490.000000\nsaldırgan      0.379310  0.392857  0.385965    28.000000\naccuracy       0.801956  0.801956  0.801956     0.801956\nmacro avg      0.622530  0.590497  0.600610  2045.000000\nweighted avg   0.788630  0.801956  0.790207  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023794 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 22848\n[LightGBM] [Info] Number of data points in the train set: 8179, number of used features: 966\n[LightGBM] [Info] Start training from score -0.277827\n[LightGBM] [Info] Start training from score -1.488549\n[LightGBM] [Info] Start training from score -4.082071\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: None, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8517\n  Precision (Ağ.): 0.8645\n  Recall (Ağ.)   : 0.8517\n  F1-Skor (Ağ.)  : 0.8286\n\nKarışıklık Matrisi:\n  6149 46 0\n  1098 747 1\n  60 8 70\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.841522  0.992575  0.910828  6195.000000\nnefret         0.932584  0.404659  0.564413  1846.000000\nsaldırgan      0.985915  0.507246  0.669856   138.000000\naccuracy       0.851693  0.851693  0.851693     0.851693\nmacro avg      0.920007  0.634827  0.715032  8179.000000\nweighted avg   0.864511  0.851693  0.828576  8179.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8137\n  Precision (Ağ.): 0.8091\n  Recall (Ağ.)   : 0.8137\n  F1-Skor (Ağ.)  : 0.7828\n\nKarışıklık Matrisi:\n  1492 34 1\n  323 164 3\n  11 9 8\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.817087  0.977079  0.889949  1527.000000\nnefret         0.792271  0.334694  0.470588   490.000000\nsaldırgan      0.666667  0.285714  0.400000    28.000000\naccuracy       0.813692  0.813692  0.813692     0.813692\nmacro avg      0.758675  0.532496  0.586846  2045.000000\nweighted avg   0.809081  0.813692  0.782758  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.488687 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 202569\n[LightGBM] [Info] Number of data points in the train set: 18585, number of used features: 6474\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTE, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9090\n  Precision (Ağ.): 0.9121\n  Recall (Ağ.)   : 0.9090\n  F1-Skor (Ağ.)  : 0.9085\n\nKarışıklık Matrisi:\n  5739 425 31\n  1125 4997 73\n  32 5 6158\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.832222  0.926392  0.876786   6195.000000\nnefret         0.920767  0.806618  0.859921   6195.000000\nsaldırgan      0.983392  0.994027  0.988681   6195.000000\naccuracy       0.909013  0.909013  0.909013      0.909013\nmacro avg      0.912127  0.909013  0.908462  18585.000000\nweighted avg   0.912127  0.909013  0.908462  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7985\n  Precision (Ağ.): 0.7857\n  Recall (Ağ.)   : 0.7985\n  F1-Skor (Ağ.)  : 0.7877\n\nKarışıklık Matrisi:\n  1395 123 9\n  254 226 10\n  9 7 12\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.841375  0.913556  0.875981  1527.000000\nnefret         0.634831  0.461224  0.534279   490.000000\nsaldırgan      0.387097  0.428571  0.406780    28.000000\naccuracy       0.798533  0.798533  0.798533     0.798533\nmacro avg      0.621101  0.601117  0.605680  2045.000000\nweighted avg   0.785666  0.798533  0.787682  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000262 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 407\n[LightGBM] [Info] Number of data points in the train set: 414, number of used features: 26\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.6981\n  Precision (Ağ.): 0.7015\n  Recall (Ağ.)   : 0.6981\n  F1-Skor (Ağ.)  : 0.6985\n\nKarışıklık Matrisi:\n  100 18 20\n  26 96 16\n  29 16 93\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.645161  0.724638  0.682594  138.000000\nnefret         0.738462  0.695652  0.716418  138.000000\nsaldırgan      0.720930  0.673913  0.696629  138.000000\naccuracy       0.698068  0.698068  0.698068    0.698068\nmacro avg      0.701518  0.698068  0.698547  414.000000\nweighted avg   0.701518  0.698068  0.698547  414.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.4538\n  Precision (Ağ.): 0.6896\n  Recall (Ağ.)   : 0.4538\n  F1-Skor (Ağ.)  : 0.5140\n\nKarışıklık Matrisi:\n  646 579 302\n  133 271 86\n  7 10 11\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score     support\nhiçbiri        0.821883  0.423052  0.558582  1527.00000\nnefret         0.315116  0.553061  0.401481   490.00000\nsaldırgan      0.027569  0.392857  0.051522    28.00000\naccuracy       0.453790  0.453790  0.453790     0.45379\nmacro avg      0.388189  0.456323  0.337195  2045.00000\nweighted avg   0.689582  0.453790  0.513997  2045.00000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.482303 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 202569\n[LightGBM] [Info] Number of data points in the train set: 18585, number of used features: 6474\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTETomek, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9090\n  Precision (Ağ.): 0.9121\n  Recall (Ağ.)   : 0.9090\n  F1-Skor (Ağ.)  : 0.9085\n\nKarışıklık Matrisi:\n  5739 425 31\n  1125 4997 73\n  32 5 6158\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.832222  0.926392  0.876786   6195.000000\nnefret         0.920767  0.806618  0.859921   6195.000000\nsaldırgan      0.983392  0.994027  0.988681   6195.000000\naccuracy       0.909013  0.909013  0.909013      0.909013\nmacro avg      0.912127  0.909013  0.908462  18585.000000\nweighted avg   0.912127  0.909013  0.908462  18585.000000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.7985\n  Precision (Ağ.): 0.7857\n  Recall (Ağ.)   : 0.7985\n  F1-Skor (Ağ.)  : 0.7877\n\nKarışıklık Matrisi:\n  1395 123 9\n  254 226 10\n  9 7 12\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.841375  0.913556  0.875981  1527.000000\nnefret         0.634831  0.461224  0.534279   490.000000\nsaldırgan      0.387097  0.428571  0.406780    28.000000\naccuracy       0.798533  0.798533  0.798533     0.798533\nmacro avg      0.621101  0.601117  0.605680  2045.000000\nweighted avg   0.785666  0.798533  0.787682  2045.000000\n\n================================================================================\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025567 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 24056\n[LightGBM] [Info] Number of data points in the train set: 8179, number of used features: 1067\n[LightGBM] [Info] Start training from score -0.277827\n[LightGBM] [Info] Start training from score -1.488549\n[LightGBM] [Info] Start training from score -4.082071\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: None, Model: LightGBM\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8510\n  Precision (Ağ.): 0.8643\n  Recall (Ağ.)   : 0.8510\n  F1-Skor (Ağ.)  : 0.8274\n\nKarışıklık Matrisi:\n  6152 43 0\n  1104 740 2\n  62 8 68\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score     support\nhiçbiri        0.840667  0.993059  0.910531  6195.00000\nnefret         0.935525  0.400867  0.561244  1846.00000\nsaldırgan      0.971429  0.492754  0.653846   138.00000\naccuracy       0.850960  0.850960  0.850960     0.85096\nmacro avg      0.915873  0.628893  0.708540  8179.00000\nweighted avg   0.864283  0.850960  0.827366  8179.00000\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8093\n  Precision (Ağ.): 0.7963\n  Recall (Ağ.)   : 0.8093\n  F1-Skor (Ağ.)  : 0.7837\n\nKarışıklık Matrisi:\n  1468 56 3\n  306 180 4\n  11 10 7\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.822409  0.961362  0.886473  1527.000000\nnefret         0.731707  0.367347  0.489130   490.000000\nsaldırgan      0.500000  0.250000  0.333333    28.000000\naccuracy       0.809291  0.809291  0.809291     0.809291\nmacro avg      0.684705  0.526236  0.569646  2045.000000\nweighted avg   0.796262  0.809291  0.783693  2045.000000\n\n================================================================================\n\n=== LightGBM İçin En İyi Kombinasyon ===\nVektörleştirici : TFIDF Unigram\nYeniden Örnekleme : None\nModel : LightGBM\nTest Accuracy : 0.8137\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\n\n# MLP (ANN)\nann = MLPClassifier(\n    hidden_layer_sizes=(50,25), \n    max_iter=100, \n    random_state=42\n)\nprint(\"\\n\\n=== MULTI-LAYER PERCEPTRON (ANN) ===\")\nbest_combination_ann = run_detailed_experiments(\n    clf=ann,\n    preprocessed_data=preprocessed_data,\n    class_names=class_names,\n    classifier_name=\"ANN\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T09:21:03.320071Z","iopub.execute_input":"2024-12-22T09:21:03.320372Z","iopub.status.idle":"2024-12-22T10:16:24.785799Z","shell.execute_reply.started":"2024-12-22T09:21:03.320350Z","shell.execute_reply":"2024-12-22T10:16:24.784597Z"}},"outputs":[{"name":"stdout","text":"\n\n=== MULTI-LAYER PERCEPTRON (ANN) ===\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTE, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9996\n  Precision (Ağ.): 0.9996\n  Recall (Ağ.)   : 0.9996\n  F1-Skor (Ağ.)  : 0.9996\n\nKarışıklık Matrisi:\n  6195 0 0\n  7 6188 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.998871  1.000000  0.999435   6195.000000\nnefret         1.000000  0.998870  0.999435   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999623  0.999623  0.999623      0.999623\nmacro avg      0.999624  0.999623  0.999623  18585.000000\nweighted avg   0.999624  0.999623  0.999623  18585.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.5738755716484836, 0.06362167378082052, 0.016626115114477844, 0.0080011489838992, 0.005399770419557215, 0.0043639916183976085, 0.003677989057102669, 0.0031843007848761182, 0.002595316846684354, 0.0026405446300549564, 0.0022834316612216844, 0.0022173866773163224, 0.0020527466763714365, 0.002418213146919841, 0.0021960521380634188, 0.0019588398000069565, 0.0017232882045913778, 0.001858000668895912, 0.0016867949624606155, 0.001984302731259298, 0.002014376225999231, 0.0014328662207438033, 0.0016303758941782012, 0.0016597478677485137, 0.001911264000437635, 0.0015393734014575073, 0.0015257422448442144, 0.001533644868079808, 0.0017846223691128297, 0.001366348046411229, 0.0014513578672578794, 0.0013327129555067644, 0.001563245985333136]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8010\n  Precision (Ağ.): 0.7842\n  Recall (Ağ.)   : 0.8010\n  F1-Skor (Ağ.)  : 0.7916\n\nKarışıklık Matrisi:\n  1373 152 2\n  223 265 2\n  13 15 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.853325  0.899149  0.875638  1527.000000\nnefret         0.613426  0.540816  0.574837   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.800978  0.800978  0.800978     0.800978\nmacro avg      0.488917  0.479988  0.483492  2045.000000\nweighted avg   0.784159  0.800978  0.791574  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[1.1281943082204766, 1.070692320470764, 1.0257537070946992, 0.9749444040888752, 0.9147719405549737, 0.8477017259820071, 0.773243900656873, 0.6944016635494532, 0.6145088574908205, 0.5355206960898666, 0.46142558256957844, 0.39425451339322015, 0.3352912881904754, 0.2840433814339592, 0.23989083149755633, 0.20308505831761064, 0.17211817050642442, 0.14681615117045416, 0.12551563328809784, 0.10796026892011293, 0.09293605095798842, 0.08065360960453605, 0.07034234399403927, 0.06145977154509457, 0.054037900520004505, 0.04792810870057719, 0.0425624855183173, 0.038109802759960655, 0.034189373397539205, 0.03070269655599686, 0.027654749041766933, 0.02518214764330122, 0.02290421579367873, 0.02103877983381207, 0.019265709928038038, 0.017775886526430286, 0.01644563878141164, 0.015306594015549922, 0.0142797011933177, 0.01339145486055365, 0.012584264666878661, 0.011864787491249003, 0.011218102729349321, 0.010634482636025563, 0.010150170747179917, 0.009667653448368617, 0.009224990086970121, 0.008799624617779311, 0.008403589047325983, 0.008024178730491278, 0.007675477373772773, 0.007366613513087305, 0.007042919370056926, 0.006758342512506218, 0.006482742318957324, 0.006227586184262078, 0.005996724173556204, 0.005771068497786774, 0.005545375206389864, 0.005317890714533663, 0.005106109795060711, 0.004916094310819239, 0.004738367396470717, 0.004579786516891586, 0.004434492573026873, 0.004294433441401823, 0.00416708377605475, 0.004047089315590939, 0.003933560657249269, 0.0038274039322627336, 0.0037189156820808633, 0.00361818158932858, 0.003511056709282352, 0.003415021835375523, 0.003327548732288218, 0.003233800772768288, 0.0031490421543950617, 0.003066100247679413, 0.002984303145826871, 0.002908581526455096, 0.002837515495242416, 0.0027657540703453303, 0.002698147198255511, 0.0026324953161144028]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5134\n  Precision (Ağ.): 0.7335\n  Recall (Ağ.)   : 0.5134\n  F1-Skor (Ağ.)  : 0.5832\n\nKarışıklık Matrisi:\n  786 432 309\n  118 244 128\n  3 5 20\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.866593  0.514735  0.645850  1527.000000\nnefret         0.358297  0.497959  0.416738   490.000000\nsaldırgan      0.043764  0.714286  0.082474    28.000000\naccuracy       0.513447  0.513447  0.513447     0.513447\nmacro avg      0.422884  0.575660  0.381688  2045.000000\nweighted avg   0.733535  0.513447  0.583239  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: SMOTETomek, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9996\n  Precision (Ağ.): 0.9996\n  Recall (Ağ.)   : 0.9996\n  F1-Skor (Ağ.)  : 0.9996\n\nKarışıklık Matrisi:\n  6195 0 0\n  7 6188 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.998871  1.000000  0.999435   6195.000000\nnefret         1.000000  0.998870  0.999435   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999623  0.999623  0.999623      0.999623\nmacro avg      0.999624  0.999623  0.999623  18585.000000\nweighted avg   0.999624  0.999623  0.999623  18585.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.5738755716484836, 0.06362167378082052, 0.016626115114477844, 0.0080011489838992, 0.005399770419557215, 0.0043639916183976085, 0.003677989057102669, 0.0031843007848761182, 0.002595316846684354, 0.0026405446300549564, 0.0022834316612216844, 0.0022173866773163224, 0.0020527466763714365, 0.002418213146919841, 0.0021960521380634188, 0.0019588398000069565, 0.0017232882045913778, 0.001858000668895912, 0.0016867949624606155, 0.001984302731259298, 0.002014376225999231, 0.0014328662207438033, 0.0016303758941782012, 0.0016597478677485137, 0.001911264000437635, 0.0015393734014575073, 0.0015257422448442144, 0.001533644868079808, 0.0017846223691128297, 0.001366348046411229, 0.0014513578672578794, 0.0013327129555067644, 0.001563245985333136]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8010\n  Precision (Ağ.): 0.7842\n  Recall (Ağ.)   : 0.8010\n  F1-Skor (Ağ.)  : 0.7916\n\nKarışıklık Matrisi:\n  1373 152 2\n  223 265 2\n  13 15 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.853325  0.899149  0.875638  1527.000000\nnefret         0.613426  0.540816  0.574837   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.800978  0.800978  0.800978     0.800978\nmacro avg      0.488917  0.479988  0.483492  2045.000000\nweighted avg   0.784159  0.800978  0.791574  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Unigram, Yeniden Örnekleme: None, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 1844 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.999677  1.000000  0.999839  6195.000000\nnefret         1.000000  0.998917  0.999458  1846.000000\nsaldırgan      1.000000  1.000000  1.000000   138.000000\naccuracy       0.999755  0.999755  0.999755     0.999755\nmacro avg      0.999892  0.999639  0.999766  8179.000000\nweighted avg   0.999756  0.999755  0.999755  8179.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.7673250704555962, 0.3715030394451938, 0.15543880566857174, 0.07164577721816898, 0.036945389497966614, 0.01853575442142512, 0.010113791133873853, 0.006719505440942495, 0.005159110668204294, 0.003955558062585004, 0.0036821568995878146, 0.0027090292939056554, 0.0030345358496497005, 0.0022662041775506027, 0.002440602810877812, 0.0018940777238124534, 0.0022058935236968745, 0.001694983446232219, 0.0019074962482222547, 0.0015507236087186692, 0.0017677042961222627, 0.0020178606400587168, 0.001109284422066468, 0.0016381001844497515, 0.0014540205764718035, 0.001583202512727155, 0.0014051267983066903, 0.001415109012230478, 0.0015406302720209922, 0.0013888061586084686, 0.0014094330416556194, 0.001612870861520131, 0.001392099806402566, 0.0014315205588822028]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8298\n  Precision (Ağ.): 0.8106\n  Recall (Ağ.)   : 0.8298\n  F1-Skor (Ağ.)  : 0.8143\n\nKarışıklık Matrisi:\n  1442 84 1\n  234 255 1\n  17 11 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.851742  0.944335  0.895652  1527.000000\nnefret         0.728571  0.520408  0.607143   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.829829  0.829829  0.829829     0.829829\nmacro avg      0.526771  0.488248  0.500932  2045.000000\nweighted avg   0.810568  0.829829  0.814260  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTE, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6194 1 0\n  3 6192 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999516  0.999839  0.999677   6195.000000\nnefret         0.999839  0.999516  0.999677   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999785  0.999785  0.999785      0.999785\nmacro avg      0.999785  0.999785  0.999785  18585.000000\nweighted avg   0.999785  0.999785  0.999785  18585.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.5217994449235112, 0.03424735637316114, 0.008803082082027189, 0.00524472880685846, 0.003687812067849166, 0.00304882786655349, 0.0026587549401137786, 0.0022953843608019913, 0.0021694388083674625, 0.002102321422792969, 0.0019496551308501684, 0.0017913278313718367, 0.0017741730635708823, 0.0017626621844706303, 0.0017111084350380714, 0.0016981366854217548, 0.001477447833371702, 0.0014629989447168263, 0.0015012646506845806, 0.0013615500829180609, 0.001341051044780054, 0.0013126971895515097, 0.001355188755836636, 0.001329504189020945, 0.0012629732554702588, 0.0012906646391923368, 0.0011950476616901442, 0.0012581889476984199, 0.0011517254583225836, 0.0012383698184452229, 0.001133394386299843]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8112\n  Precision (Ağ.): 0.7938\n  Recall (Ağ.)   : 0.8112\n  F1-Skor (Ağ.)  : 0.8014\n\nKarışıklık Matrisi:\n  1382 144 1\n  213 277 0\n  18 10 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.856789  0.905043  0.880255  1527.000000\nnefret         0.642691  0.565306  0.601520   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.811247  0.811247  0.811247     0.811247\nmacro avg      0.499827  0.490116  0.493925  2045.000000\nweighted avg   0.793758  0.811247  0.801415  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[1.1186093498865188, 1.0154685529508454, 0.9272085940085746, 0.8383185388550091, 0.7499559696501003, 0.6629745051704743, 0.5836232448679238, 0.5144484558178032, 0.45502283285977185, 0.4035948725834561, 0.3593744203692472, 0.3196564168599493, 0.2845872734538023, 0.252340281063635, 0.22300697807603412, 0.19637018261854194, 0.1714080054420204, 0.14920516013143142, 0.1291539055938882, 0.11151041290581515, 0.09648583319066227, 0.08314918458398414, 0.07207679461856971, 0.06283624688116249, 0.05491512791995265, 0.04839091974214656, 0.042566183686256406, 0.037749379976072174, 0.03360445727322989, 0.029937276696294975, 0.026873070417107014, 0.024249482942232187, 0.022102482283403332, 0.020288742865917184, 0.018644330290665374, 0.017263010885289327, 0.015862064447149563, 0.014691465024556514, 0.013694655994171107, 0.0128181888518702, 0.012044740392073339, 0.011351270711162816, 0.010730432121419676, 0.010142349757002171, 0.009560364264739309, 0.009053300884668379, 0.00857260724346419, 0.008091348270632796, 0.0076319248216163696, 0.007196345601738361, 0.006782461517905267, 0.006402692029610779, 0.006064522102000057, 0.005755418845017751, 0.0054512539509413895, 0.005168839657663003, 0.004916546291760777, 0.0046738883042105155, 0.004453384436285439, 0.0042560887406845604, 0.0040586871162164905, 0.003868061544123479, 0.0037015595673650934, 0.003522658505762257, 0.0033606415044282367, 0.0032142902310055817, 0.003072595307941814, 0.0029401255506104314, 0.002824184651742127, 0.0027141495076570534, 0.0026172792118096693, 0.002511155540427724, 0.0024097938556458063, 0.0023189618958079296, 0.0022305068570634594, 0.0021484431356515576, 0.0020632491248241368, 0.0019933465677327007, 0.0019168960668596521, 0.001854492684698479, 0.0017938041565354896, 0.0017399990107054295, 0.0016862233208141464, 0.001638144804566091]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.4528\n  Precision (Ağ.): 0.7459\n  Recall (Ağ.)   : 0.4528\n  F1-Skor (Ağ.)  : 0.4963\n\nKarışıklık Matrisi:\n  560 727 240\n  63 352 75\n  3 11 14\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.894569  0.366732  0.520204  1527.000000\nnefret         0.322936  0.718367  0.445570   490.000000\nsaldırgan      0.042553  0.500000  0.078431    28.000000\naccuracy       0.452812  0.452812  0.452812     0.452812\nmacro avg      0.420019  0.528367  0.348068  2045.000000\nweighted avg   0.745935  0.452812  0.496272  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: SMOTETomek, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6194 1 0\n  3 6192 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999516  0.999839  0.999677   6195.000000\nnefret         0.999839  0.999516  0.999677   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999785  0.999785  0.999785      0.999785\nmacro avg      0.999785  0.999785  0.999785  18585.000000\nweighted avg   0.999785  0.999785  0.999785  18585.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.5217994449235112, 0.03424735637316114, 0.008803082082027189, 0.00524472880685846, 0.003687812067849166, 0.00304882786655349, 0.0026587549401137786, 0.0022953843608019913, 0.0021694388083674625, 0.002102321422792969, 0.0019496551308501684, 0.0017913278313718367, 0.0017741730635708823, 0.0017626621844706303, 0.0017111084350380714, 0.0016981366854217548, 0.001477447833371702, 0.0014629989447168263, 0.0015012646506845806, 0.0013615500829180609, 0.001341051044780054, 0.0013126971895515097, 0.001355188755836636, 0.001329504189020945, 0.0012629732554702588, 0.0012906646391923368, 0.0011950476616901442, 0.0012581889476984199, 0.0011517254583225836, 0.0012383698184452229, 0.001133394386299843]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8112\n  Precision (Ağ.): 0.7938\n  Recall (Ağ.)   : 0.8112\n  F1-Skor (Ağ.)  : 0.8014\n\nKarışıklık Matrisi:\n  1382 144 1\n  213 277 0\n  18 10 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.856789  0.905043  0.880255  1527.000000\nnefret         0.642691  0.565306  0.601520   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.811247  0.811247  0.811247     0.811247\nmacro avg      0.499827  0.490116  0.493925  2045.000000\nweighted avg   0.793758  0.811247  0.801415  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: Count Bigram, Yeniden Örnekleme: None, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6194 1 0\n  1 1845 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.999839  0.999839  0.999839  6195.000000\nnefret         0.999458  0.999458  0.999458  1846.000000\nsaldırgan      1.000000  1.000000  1.000000   138.000000\naccuracy       0.999755  0.999755  0.999755     0.999755\nmacro avg      0.999766  0.999766  0.999766  8179.000000\nweighted avg   0.999755  0.999755  0.999755  8179.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.7628210806316292, 0.2818019775620123, 0.09581118561279395, 0.042824196570737834, 0.0251657267117066, 0.010979491035695444, 0.005225640791114583, 0.0032491326722674328, 0.0026172354446207776, 0.0020869879525263683, 0.001784269863677529, 0.0014854644125562263, 0.0013706142400395159, 0.0013457150838022084, 0.0014044499841802116, 0.0012130387282579945, 0.001291008096495385, 0.0011255511342870648, 0.0011419998337276144, 0.0010622986270299472, 0.001046481277297639, 0.0010211011886784335, 0.0010102452385032407, 0.0009974812186096626, 0.0009821824024445868, 0.001030001340548324, 0.0010220165546457414]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8215\n  Precision (Ağ.): 0.8029\n  Recall (Ağ.)   : 0.8215\n  F1-Skor (Ağ.)  : 0.8093\n\nKarışıklık Matrisi:\n  1412 112 3\n  222 268 0\n  18 10 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.854722  0.924689  0.888330  1527.000000\nnefret         0.687179  0.546939  0.609091   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.821516  0.821516  0.821516     0.821516\nmacro avg      0.513967  0.490543  0.499140  2045.000000\nweighted avg   0.802874  0.821516  0.809259  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTE, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9994\n  Precision (Ağ.): 0.9994\n  Recall (Ağ.)   : 0.9994\n  F1-Skor (Ağ.)  : 0.9994\n\nKarışıklık Matrisi:\n  6185 10 0\n  1 6194 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999838  0.998386  0.999112   6195.000000\nnefret         0.998388  0.999839  0.999113   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999408  0.999408  0.999408      0.999408\nmacro avg      0.999409  0.999408  0.999408  18585.000000\nweighted avg   0.999409  0.999408  0.999408  18585.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.8189540628262543, 0.16731059112469937, 0.03370971568428062, 0.012483087389217063, 0.006994527442078774, 0.0051443527243898715, 0.004264799007373951, 0.003769404976376289, 0.003372011492458889, 0.0032626439799767278, 0.0029838240688957454, 0.0027019949185011164, 0.0025842442972373986, 0.002674095097597742, 0.0025256028529944716, 0.0025206062834106343, 0.002355473540922408, 0.002305568630454327, 0.0022560613623143597, 0.002337009591749833, 0.0023328176910478904, 0.0021308103335432893, 0.002168431436115771, 0.0021885696981476465, 0.002199396059146356, 0.002098229109990299, 0.002016257263962015, 0.002082014573168892, 0.002033278949913203, 0.001956566838918754, 0.001903746358570166, 0.00195147067210512, 0.0019500530785211943]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8205\n  Precision (Ağ.): 0.8060\n  Recall (Ağ.)   : 0.8205\n  F1-Skor (Ağ.)  : 0.8126\n\nKarışıklık Matrisi:\n  1383 141 3\n  194 295 1\n  17 11 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.867629  0.905697  0.886254  1527.000000\nnefret         0.659955  0.602041  0.629669   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.820538  0.820538  0.820538     0.820538\nmacro avg      0.509195  0.502579  0.505308  2045.000000\nweighted avg   0.805989  0.820538  0.812640  2045.000000\n\n================================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: RandomUnderSampler, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[1.1309629415579465, 1.1165655658256604, 1.1036399010192945, 1.0886781982316485, 1.071889053660372, 1.052745123689071, 1.0308001298877354, 1.0061406243424487, 0.978868760364355, 0.9482562114372921, 0.9140788558030474, 0.876717112470134, 0.836861561877025, 0.794941747303228, 0.7518631518791841, 0.707707704606206, 0.6632737334599241, 0.6190664939154173, 0.5747548676633604, 0.5301675373661345, 0.486171743434178, 0.4424966565323337, 0.40042863377027466, 0.36016010721155983, 0.32189741678076667, 0.2864441169069585, 0.25420308749186243, 0.2252192525660934, 0.19928990736048002, 0.17590492761100546, 0.15561439459703968, 0.13786002532474084, 0.12204583880354242, 0.10854537749866357, 0.09648083607695529, 0.08614916772370176, 0.07713755543606293, 0.06944514265331093, 0.06268887980312542, 0.05686929060859957, 0.05181242674237863, 0.04747104845173693, 0.04359559676428347, 0.04021121126637943, 0.03728708536383035, 0.03461647098098976, 0.03222014284859533, 0.03003530086065836, 0.028079942051684802, 0.026326143045241134, 0.024730210393292894, 0.02328746242482881, 0.02192533840706959, 0.02070247032204688, 0.01957319722878184, 0.01852540777397617, 0.017586049745278658, 0.016704544509205842, 0.015869061718125275, 0.015090401948592513, 0.01436888878143928, 0.013714951941414158, 0.01310727230776911, 0.012545460313875316, 0.01203059918978364, 0.011542079714070196, 0.011084787288610487, 0.010660511660489484, 0.01026441042918514, 0.009899038630954309, 0.009545827147753342, 0.009216576331911457, 0.00889774850330491, 0.008604067241281703, 0.008328251147903682, 0.008060319942351124, 0.0078088647511558255, 0.007572498962182353, 0.007350121987729832, 0.007138216074394144, 0.006937009589798785, 0.006741711944255276, 0.0065532757650538925, 0.006374368439118067, 0.006196036121148418, 0.00603102154224967, 0.005871069940132795, 0.005722277385889043, 0.005573966636980214, 0.0054355897493984385, 0.00530502629938045, 0.00517593679446817, 0.005058272879797479, 0.00494199194562608, 0.00482814351392253, 0.00471990017749837, 0.004612080180472222, 0.004509605413185801, 0.004410134642653995, 0.004315875821369858]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5330\n  Precision (Ağ.): 0.7274\n  Recall (Ağ.)   : 0.5330\n  F1-Skor (Ağ.)  : 0.6003\n\nKarışıklık Matrisi:\n  846 402 279\n  134 225 131\n  5 4 19\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.858883  0.554028  0.673567  1527.000000\nnefret         0.356577  0.459184  0.401427   490.000000\nsaldırgan      0.044289  0.678571  0.083151    28.000000\naccuracy       0.533007  0.533007  0.533007     0.533007\nmacro avg      0.419916  0.563928  0.386048  2045.000000\nweighted avg   0.727373  0.533007  0.600276  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: SMOTETomek, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9994\n  Precision (Ağ.): 0.9994\n  Recall (Ağ.)   : 0.9994\n  F1-Skor (Ağ.)  : 0.9994\n\nKarışıklık Matrisi:\n  6185 10 0\n  1 6194 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999838  0.998386  0.999112   6195.000000\nnefret         0.998388  0.999839  0.999113   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999408  0.999408  0.999408      0.999408\nmacro avg      0.999409  0.999408  0.999408  18585.000000\nweighted avg   0.999409  0.999408  0.999408  18585.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.8189540628262543, 0.16731059112469937, 0.03370971568428062, 0.012483087389217063, 0.006994527442078774, 0.0051443527243898715, 0.004264799007373951, 0.003769404976376289, 0.003372011492458889, 0.0032626439799767278, 0.0029838240688957454, 0.0027019949185011164, 0.0025842442972373986, 0.002674095097597742, 0.0025256028529944716, 0.0025206062834106343, 0.002355473540922408, 0.002305568630454327, 0.0022560613623143597, 0.002337009591749833, 0.0023328176910478904, 0.0021308103335432893, 0.002168431436115771, 0.0021885696981476465, 0.002199396059146356, 0.002098229109990299, 0.002016257263962015, 0.002082014573168892, 0.002033278949913203, 0.001956566838918754, 0.001903746358570166, 0.00195147067210512, 0.0019500530785211943]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8205\n  Precision (Ağ.): 0.8060\n  Recall (Ağ.)   : 0.8205\n  F1-Skor (Ağ.)  : 0.8126\n\nKarışıklık Matrisi:\n  1383 141 3\n  194 295 1\n  17 11 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.867629  0.905697  0.886254  1527.000000\nnefret         0.659955  0.602041  0.629669   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.820538  0.820538  0.820538     0.820538\nmacro avg      0.509195  0.502579  0.505308  2045.000000\nweighted avg   0.805989  0.820538  0.812640  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Unigram, Yeniden Örnekleme: None, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 1844 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.999677  1.000000  0.999839  6195.000000\nnefret         1.000000  0.998917  0.999458  1846.000000\nsaldırgan      1.000000  1.000000  1.000000   138.000000\naccuracy       0.999755  0.999755  0.999755     0.999755\nmacro avg      0.999892  0.999639  0.999766  8179.000000\nweighted avg   0.999756  0.999755  0.999755  8179.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.881770272991832, 0.5919414611396063, 0.3702196247328114, 0.18236691194939603, 0.09411980264597296, 0.06160165665651362, 0.04710483333397822, 0.03781732212127982, 0.030795899211895433, 0.024337670642703764, 0.017603347946906937, 0.0112779643493463, 0.007482455842613607, 0.005253863321042669, 0.004208584798485685, 0.00338223705928969, 0.002970095589665507, 0.0025244529933855986, 0.002343898525556065, 0.002080922044809443, 0.0020076963728572297, 0.0021114036454637298, 0.0017121464106910954, 0.0018392167981059499, 0.0016912323499530435, 0.001687635838419931, 0.0016223072100959525, 0.0016301409922444537, 0.00163780956346032, 0.0016110571037886386, 0.0015662119875758471, 0.0016014491124463732, 0.0015303728294399522, 0.0015196737447032312]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8289\n  Precision (Ağ.): 0.8092\n  Recall (Ağ.)   : 0.8289\n  F1-Skor (Ağ.)  : 0.8140\n\nKarışıklık Matrisi:\n  1435 91 1\n  230 260 0\n  18 10 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.852644  0.939751  0.894081  1527.000000\nnefret         0.720222  0.530612  0.611046   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.828851  0.828851  0.828851     0.828851\nmacro avg      0.524289  0.490121  0.501709  2045.000000\nweighted avg   0.809240  0.828851  0.814022  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTE, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9994\n  Precision (Ağ.): 0.9994\n  Recall (Ağ.)   : 0.9994\n  F1-Skor (Ağ.)  : 0.9994\n\nKarışıklık Matrisi:\n  6185 10 0\n  1 6194 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999838  0.998386  0.999112   6195.000000\nnefret         0.998388  0.999839  0.999113   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999408  0.999408  0.999408      0.999408\nmacro avg      0.999409  0.999408  0.999408  18585.000000\nweighted avg   0.999409  0.999408  0.999408  18585.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.7788098314573345, 0.1416253132111173, 0.02112329817929959, 0.009701063041077655, 0.006171088820041301, 0.0045673352276127555, 0.0038124561981684045, 0.0033510876153783516, 0.0032192576715431133, 0.0031540958440743187, 0.0029739965414128573, 0.002834612818756249, 0.0027719041151396838, 0.002794318776666588, 0.002745745655178447, 0.002810743165997397, 0.002511446980300339, 0.0025928235946572026, 0.0025974781398406615, 0.002417519525113945, 0.002469205619656402, 0.0023635947957764813, 0.002420950934739886, 0.002498966220303476, 0.0022645337877123316, 0.002299057072333537, 0.0022231963133925, 0.0023522913284292303]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8367\n  Precision (Ağ.): 0.8186\n  Recall (Ağ.)   : 0.8367\n  F1-Skor (Ağ.)  : 0.8237\n\nKarışıklık Matrisi:\n  1435 90 2\n  214 276 0\n  17 11 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.861345  0.939751  0.898841  1527.000000\nnefret         0.732095  0.563265  0.636678   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.836675  0.836675  0.836675     0.836675\nmacro avg      0.531147  0.501005  0.511840  2045.000000\nweighted avg   0.818582  0.836675  0.823718  2045.000000\n\n================================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: RandomUnderSampler, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 1.0000\n  Precision (Ağ.): 1.0000\n  Recall (Ağ.)   : 1.0000\n  F1-Skor (Ağ.)  : 1.0000\n\nKarışıklık Matrisi:\n  138 0 0\n  0 138 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision  recall  f1-score  support\nhiçbiri             1.0     1.0       1.0    138.0\nnefret              1.0     1.0       1.0    138.0\nsaldırgan           1.0     1.0       1.0    138.0\naccuracy            1.0     1.0       1.0      1.0\nmacro avg           1.0     1.0       1.0    414.0\nweighted avg        1.0     1.0       1.0    414.0\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[1.118611343927199, 1.09830546208741, 1.0774102380357502, 1.0532042917494036, 1.0251451301875898, 0.993802947729394, 0.958441111067931, 0.9193318670635641, 0.8786034438759233, 0.8365865647980556, 0.7933437218493886, 0.749918556469247, 0.7067193743379796, 0.664316653892038, 0.6233417571943164, 0.5835984932466405, 0.5457293846297956, 0.5094481205249753, 0.47498626162539354, 0.4423093823309106, 0.4113247021702753, 0.3822119535057441, 0.3545354713412875, 0.3283697014460242, 0.30390533314643847, 0.28082650151753774, 0.25934692321658714, 0.23884772856229744, 0.21980851789620764, 0.20161435285466878, 0.18477541577908152, 0.16908297629172103, 0.1547157981339284, 0.1412969700622098, 0.1289286833451566, 0.11732358903222614, 0.10646541398187767, 0.0966469961453175, 0.08773104752100608, 0.07982566768115268, 0.07259558861324752, 0.06623690512047874, 0.06075882326923131, 0.055578433232722074, 0.05096161131789719, 0.046832853384927854, 0.04313379151608057, 0.03974661749964175, 0.03680017807385772, 0.03409333135096923, 0.03175979917636816, 0.02965540860914378, 0.02781940685648849, 0.02610338915747721, 0.024562132023152522, 0.023153685644225794, 0.02187846958596925, 0.02069534124182042, 0.019648214504108336, 0.01869693961494787, 0.017802018740211707, 0.016972988520958573, 0.016206161115595683, 0.015452270586536703, 0.014769530005443498, 0.014134753240652132, 0.013536498740675367, 0.01298282179567549, 0.01248785949467461, 0.012018636543624067, 0.01159055212309971, 0.011169255341020761, 0.01077366925570124, 0.010408316182629499, 0.010052523491060101, 0.009714412829870187, 0.009379342444622574, 0.00908100751068281, 0.008788023560634557, 0.008515093611864652, 0.008257163137341468, 0.008017073770536893, 0.0077903975489634816, 0.007574575434473977, 0.007371667000809729, 0.007168762002071898, 0.006974739558080544, 0.006780938488619339, 0.006597478373528678, 0.006423556920497314, 0.006258504278950645, 0.006104355477739649, 0.005951333148899862, 0.005803593659458529, 0.005664145249272314, 0.005524529075046669, 0.005397756438497185, 0.005271335556800815, 0.0051521003731063025, 0.00504283272134509]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.5819\n  Precision (Ağ.): 0.7246\n  Recall (Ağ.)   : 0.5819\n  F1-Skor (Ağ.)  : 0.6413\n\nKarışıklık Matrisi:\n  1012 230 285\n  188 162 140\n  6 6 16\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.839138  0.662737  0.740578  1527.000000\nnefret         0.407035  0.330612  0.364865   490.000000\nsaldırgan      0.036281  0.571429  0.068230    28.000000\naccuracy       0.581907  0.581907  0.581907     0.581907\nmacro avg      0.427485  0.521593  0.391224  2045.000000\nweighted avg   0.724609  0.581907  0.641348  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: SMOTETomek, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9994\n  Precision (Ağ.): 0.9994\n  Recall (Ağ.)   : 0.9994\n  F1-Skor (Ağ.)  : 0.9994\n\nKarışıklık Matrisi:\n  6185 10 0\n  1 6194 0\n  0 0 6195\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score       support\nhiçbiri        0.999838  0.998386  0.999112   6195.000000\nnefret         0.998388  0.999839  0.999113   6195.000000\nsaldırgan      1.000000  1.000000  1.000000   6195.000000\naccuracy       0.999408  0.999408  0.999408      0.999408\nmacro avg      0.999409  0.999408  0.999408  18585.000000\nweighted avg   0.999409  0.999408  0.999408  18585.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.7788098314573345, 0.1416253132111173, 0.02112329817929959, 0.009701063041077655, 0.006171088820041301, 0.0045673352276127555, 0.0038124561981684045, 0.0033510876153783516, 0.0032192576715431133, 0.0031540958440743187, 0.0029739965414128573, 0.002834612818756249, 0.0027719041151396838, 0.002794318776666588, 0.002745745655178447, 0.002810743165997397, 0.002511446980300339, 0.0025928235946572026, 0.0025974781398406615, 0.002417519525113945, 0.002469205619656402, 0.0023635947957764813, 0.002420950934739886, 0.002498966220303476, 0.0022645337877123316, 0.002299057072333537, 0.0022231963133925, 0.0023522913284292303]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8367\n  Precision (Ağ.): 0.8186\n  Recall (Ağ.)   : 0.8367\n  F1-Skor (Ağ.)  : 0.8237\n\nKarışıklık Matrisi:\n  1435 90 2\n  214 276 0\n  17 11 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.861345  0.939751  0.898841  1527.000000\nnefret         0.732095  0.563265  0.636678   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.836675  0.836675  0.836675     0.836675\nmacro avg      0.531147  0.501005  0.511840  2045.000000\nweighted avg   0.818582  0.836675  0.823718  2045.000000\n\n================================================================================\n\n================================================================================\nDetaylı Rapor - Vektörleştirici: TFIDF Bigram, Yeniden Örnekleme: None, Model: ANN\n================================================================================\n\n[EĞİTİM (TRAIN) SONUÇLARI]\nEğitim Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.9998\n  Precision (Ağ.): 0.9998\n  Recall (Ağ.)   : 0.9998\n  F1-Skor (Ağ.)  : 0.9998\n\nKarışıklık Matrisi:\n  6195 0 0\n  2 1844 0\n  0 0 138\n\nSınıf bazlı metrikler (EĞİTİM):\n              precision    recall  f1-score      support\nhiçbiri        0.999677  1.000000  0.999839  6195.000000\nnefret         1.000000  0.998917  0.999458  1846.000000\nsaldırgan      1.000000  1.000000  1.000000   138.000000\naccuracy       0.999755  0.999755  0.999755     0.999755\nmacro avg      0.999892  0.999639  0.999766  8179.000000\nweighted avg   0.999756  0.999755  0.999755  8179.000000\n\nMLP Eğitim Kayıp Eğrisi (Loss Curve):\n[0.9022010856749363, 0.5720346253657285, 0.2890872823354011, 0.13446268665351846, 0.0645177419036565, 0.040948332389174384, 0.029084737366195237, 0.018916699353490465, 0.011801102645972315, 0.007719627097658955, 0.005617690931787291, 0.004368673831245576, 0.0036950970421765544, 0.0034258939571435412, 0.0029949574083785783, 0.002669233950292996, 0.0027750650984600506, 0.0023867281478663098, 0.002706334721894876, 0.0021293947478191293, 0.0022263868061804452, 0.0019885388172048864, 0.0019477479129530481, 0.0019104354404043287, 0.0017975959595629399, 0.0020110990839207693, 0.0019326798003027994, 0.0019016919288414882, 0.0017964142316316742, 0.0017900798878498923, 0.0017393249486175987, 0.0016855444607801432, 0.0015854980936423097, 0.0016377701445734437, 0.0018003271716707768, 0.0016665568993532187, 0.0016973627124137027, 0.0016280136297582516, 0.0016818526840476685, 0.0016858812153918744, 0.0014838459961020666, 0.0016272724387246073, 0.0015446035836739593, 0.0015141542921786725, 0.001661194835403377, 0.0015025738009607488, 0.0015906416814270978, 0.001501660128920334, 0.0013809142867868636, 0.0014844360444339942, 0.0012971723527827584, 0.0015150273182934601, 0.0014633721414431047, 0.0021026339844760107, 0.00152901687798902, 0.0014022513545562242, 0.0014632908699302306, 0.0014961652034786372, 0.0014753395205842554, 0.0012941980542694536]\n\n[TEST SONUÇLARI]\nTest Verisi Sonuçları:\n  Doğruluk (Accuracy): 0.8430\n  Precision (Ağ.): 0.8335\n  Recall (Ağ.)   : 0.8430\n  F1-Skor (Ağ.)  : 0.8219\n\nKarışıklık Matrisi:\n  1491 33 3\n  257 233 0\n  20 8 0\n\nSınıf bazlı metrikler (TEST):\n              precision    recall  f1-score      support\nhiçbiri        0.843326  0.976424  0.905008  1527.000000\nnefret         0.850365  0.475510  0.609948   490.000000\nsaldırgan      0.000000  0.000000  0.000000    28.000000\naccuracy       0.843032  0.843032  0.843032     0.843032\nmacro avg      0.564564  0.483978  0.504985  2045.000000\nweighted avg   0.833466  0.843032  0.821917  2045.000000\n\n================================================================================\n\n=== ANN İçin En İyi Kombinasyon ===\nVektörleştirici : TFIDF Bigram\nYeniden Örnekleme : None\nModel : ANN\nTest Accuracy : 0.8430\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"results_df = pd.DataFrame(final_results)\nresults_df_T = results_df.T\n\nresults_df_T.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:18:02.262694Z","iopub.execute_input":"2024-12-22T10:18:02.262979Z","iopub.status.idle":"2024-12-22T10:18:02.276873Z","shell.execute_reply.started":"2024-12-22T10:18:02.262957Z","shell.execute_reply":"2024-12-22T10:18:02.275956Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                    Accuracy  \\\nLogistic Regression + Count Unigram + SMOTE         0.829829   \nLogistic Regression + Count Unigram + RandomUnd...  0.562836   \nLogistic Regression + Count Unigram + SMOTETomek    0.829829   \nLogistic Regression + Count Unigram + None          0.839120   \nLogistic Regression + Count Bigram + SMOTE          0.827384   \n\n                                                    Precision(Weighted)  \\\nLogistic Regression + Count Unigram + SMOTE                    0.823114   \nLogistic Regression + Count Unigram + RandomUnd...             0.727430   \nLogistic Regression + Count Unigram + SMOTETomek               0.823114   \nLogistic Regression + Count Unigram + None                     0.826892   \nLogistic Regression + Count Bigram + SMOTE                     0.815703   \n\n                                                    Recall(Weighted)  \\\nLogistic Regression + Count Unigram + SMOTE                 0.829829   \nLogistic Regression + Count Unigram + RandomUnd...          0.562836   \nLogistic Regression + Count Unigram + SMOTETomek            0.829829   \nLogistic Regression + Count Unigram + None                  0.839120   \nLogistic Regression + Count Bigram + SMOTE                  0.827384   \n\n                                                    F1-Score(Weighted)  \nLogistic Regression + Count Unigram + SMOTE                   0.825588  \nLogistic Regression + Count Unigram + RandomUnd...            0.614660  \nLogistic Regression + Count Unigram + SMOTETomek              0.825588  \nLogistic Regression + Count Unigram + None                    0.820806  \nLogistic Regression + Count Bigram + SMOTE                    0.817889  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Precision(Weighted)</th>\n      <th>Recall(Weighted)</th>\n      <th>F1-Score(Weighted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression + Count Unigram + SMOTE</th>\n      <td>0.829829</td>\n      <td>0.823114</td>\n      <td>0.829829</td>\n      <td>0.825588</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Unigram + RandomUnderSampler</th>\n      <td>0.562836</td>\n      <td>0.727430</td>\n      <td>0.562836</td>\n      <td>0.614660</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Unigram + SMOTETomek</th>\n      <td>0.829829</td>\n      <td>0.823114</td>\n      <td>0.829829</td>\n      <td>0.825588</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Unigram + None</th>\n      <td>0.839120</td>\n      <td>0.826892</td>\n      <td>0.839120</td>\n      <td>0.820806</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Bigram + SMOTE</th>\n      <td>0.827384</td>\n      <td>0.815703</td>\n      <td>0.827384</td>\n      <td>0.817889</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model_name = \"Logistic Regression\"\n\nlr_df = results_df_T.loc[\n    results_df_T.index.str.startswith(model_name)\n]\nlr_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:18:03.972067Z","iopub.execute_input":"2024-12-22T10:18:03.972358Z","iopub.status.idle":"2024-12-22T10:18:03.983481Z","shell.execute_reply.started":"2024-12-22T10:18:03.972327Z","shell.execute_reply":"2024-12-22T10:18:03.982769Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                    Accuracy  \\\nLogistic Regression + Count Unigram + SMOTE         0.829829   \nLogistic Regression + Count Unigram + RandomUnd...  0.562836   \nLogistic Regression + Count Unigram + SMOTETomek    0.829829   \nLogistic Regression + Count Unigram + None          0.839120   \nLogistic Regression + Count Bigram + SMOTE          0.827384   \nLogistic Regression + Count Bigram + RandomUnde...  0.555501   \nLogistic Regression + Count Bigram + SMOTETomek     0.827384   \nLogistic Regression + Count Bigram + None           0.828362   \nLogistic Regression + TFIDF Unigram + SMOTE         0.828851   \nLogistic Regression + TFIDF Unigram + RandomUnd...  0.576528   \nLogistic Regression + TFIDF Unigram + SMOTETomek    0.828851   \nLogistic Regression + TFIDF Unigram + None          0.823961   \nLogistic Regression + TFIDF Bigram + SMOTE          0.830807   \nLogistic Regression + TFIDF Bigram + RandomUnde...  0.533496   \nLogistic Regression + TFIDF Bigram + SMOTETomek     0.830807   \nLogistic Regression + TFIDF Bigram + None           0.824450   \n\n                                                    Precision(Weighted)  \\\nLogistic Regression + Count Unigram + SMOTE                    0.823114   \nLogistic Regression + Count Unigram + RandomUnd...             0.727430   \nLogistic Regression + Count Unigram + SMOTETomek               0.823114   \nLogistic Regression + Count Unigram + None                     0.826892   \nLogistic Regression + Count Bigram + SMOTE                     0.815703   \nLogistic Regression + Count Bigram + RandomUnde...             0.719114   \nLogistic Regression + Count Bigram + SMOTETomek                0.815703   \nLogistic Regression + Count Bigram + None                      0.821494   \nLogistic Regression + TFIDF Unigram + SMOTE                    0.823977   \nLogistic Regression + TFIDF Unigram + RandomUnd...             0.739930   \nLogistic Regression + TFIDF Unigram + SMOTETomek               0.823977   \nLogistic Regression + TFIDF Unigram + None                     0.815113   \nLogistic Regression + TFIDF Bigram + SMOTE                     0.821321   \nLogistic Regression + TFIDF Bigram + RandomUnde...             0.746910   \nLogistic Regression + TFIDF Bigram + SMOTETomek                0.821321   \nLogistic Regression + TFIDF Bigram + None                      0.816706   \n\n                                                    Recall(Weighted)  \\\nLogistic Regression + Count Unigram + SMOTE                 0.829829   \nLogistic Regression + Count Unigram + RandomUnd...          0.562836   \nLogistic Regression + Count Unigram + SMOTETomek            0.829829   \nLogistic Regression + Count Unigram + None                  0.839120   \nLogistic Regression + Count Bigram + SMOTE                  0.827384   \nLogistic Regression + Count Bigram + RandomUnde...          0.555501   \nLogistic Regression + Count Bigram + SMOTETomek             0.827384   \nLogistic Regression + Count Bigram + None                   0.828362   \nLogistic Regression + TFIDF Unigram + SMOTE                 0.828851   \nLogistic Regression + TFIDF Unigram + RandomUnd...          0.576528   \nLogistic Regression + TFIDF Unigram + SMOTETomek            0.828851   \nLogistic Regression + TFIDF Unigram + None                  0.823961   \nLogistic Regression + TFIDF Bigram + SMOTE                  0.830807   \nLogistic Regression + TFIDF Bigram + RandomUnde...          0.533496   \nLogistic Regression + TFIDF Bigram + SMOTETomek             0.830807   \nLogistic Regression + TFIDF Bigram + None                   0.824450   \n\n                                                    F1-Score(Weighted)  \nLogistic Regression + Count Unigram + SMOTE                   0.825588  \nLogistic Regression + Count Unigram + RandomUnd...            0.614660  \nLogistic Regression + Count Unigram + SMOTETomek              0.825588  \nLogistic Regression + Count Unigram + None                    0.820806  \nLogistic Regression + Count Bigram + SMOTE                    0.817889  \nLogistic Regression + Count Bigram + RandomUnde...            0.606229  \nLogistic Regression + Count Bigram + SMOTETomek               0.817889  \nLogistic Regression + Count Bigram + None                     0.802628  \nLogistic Regression + TFIDF Unigram + SMOTE                   0.825857  \nLogistic Regression + TFIDF Unigram + RandomUnd...            0.630542  \nLogistic Regression + TFIDF Unigram + SMOTETomek              0.825857  \nLogistic Regression + TFIDF Unigram + None                    0.792900  \nLogistic Regression + TFIDF Bigram + SMOTE                    0.824894  \nLogistic Regression + TFIDF Bigram + RandomUnde...            0.596161  \nLogistic Regression + TFIDF Bigram + SMOTETomek               0.824894  \nLogistic Regression + TFIDF Bigram + None                     0.793298  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Precision(Weighted)</th>\n      <th>Recall(Weighted)</th>\n      <th>F1-Score(Weighted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression + Count Unigram + SMOTE</th>\n      <td>0.829829</td>\n      <td>0.823114</td>\n      <td>0.829829</td>\n      <td>0.825588</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Unigram + RandomUnderSampler</th>\n      <td>0.562836</td>\n      <td>0.727430</td>\n      <td>0.562836</td>\n      <td>0.614660</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Unigram + SMOTETomek</th>\n      <td>0.829829</td>\n      <td>0.823114</td>\n      <td>0.829829</td>\n      <td>0.825588</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Unigram + None</th>\n      <td>0.839120</td>\n      <td>0.826892</td>\n      <td>0.839120</td>\n      <td>0.820806</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Bigram + SMOTE</th>\n      <td>0.827384</td>\n      <td>0.815703</td>\n      <td>0.827384</td>\n      <td>0.817889</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Bigram + RandomUnderSampler</th>\n      <td>0.555501</td>\n      <td>0.719114</td>\n      <td>0.555501</td>\n      <td>0.606229</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Bigram + SMOTETomek</th>\n      <td>0.827384</td>\n      <td>0.815703</td>\n      <td>0.827384</td>\n      <td>0.817889</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + Count Bigram + None</th>\n      <td>0.828362</td>\n      <td>0.821494</td>\n      <td>0.828362</td>\n      <td>0.802628</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + TFIDF Unigram + SMOTE</th>\n      <td>0.828851</td>\n      <td>0.823977</td>\n      <td>0.828851</td>\n      <td>0.825857</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + TFIDF Unigram + RandomUnderSampler</th>\n      <td>0.576528</td>\n      <td>0.739930</td>\n      <td>0.576528</td>\n      <td>0.630542</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + TFIDF Unigram + SMOTETomek</th>\n      <td>0.828851</td>\n      <td>0.823977</td>\n      <td>0.828851</td>\n      <td>0.825857</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + TFIDF Unigram + None</th>\n      <td>0.823961</td>\n      <td>0.815113</td>\n      <td>0.823961</td>\n      <td>0.792900</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + TFIDF Bigram + SMOTE</th>\n      <td>0.830807</td>\n      <td>0.821321</td>\n      <td>0.830807</td>\n      <td>0.824894</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + TFIDF Bigram + RandomUnderSampler</th>\n      <td>0.533496</td>\n      <td>0.746910</td>\n      <td>0.533496</td>\n      <td>0.596161</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + TFIDF Bigram + SMOTETomek</th>\n      <td>0.830807</td>\n      <td>0.821321</td>\n      <td>0.830807</td>\n      <td>0.824894</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression + TFIDF Bigram + None</th>\n      <td>0.824450</td>\n      <td>0.816706</td>\n      <td>0.824450</td>\n      <td>0.793298</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"model_name = \"Random Forest\"\n\nrf_df = results_df_T.loc[\n    results_df_T.index.str.startswith(model_name)\n]\nrf_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T23:44:03.243490Z","iopub.execute_input":"2024-12-21T23:44:03.243810Z","iopub.status.idle":"2024-12-21T23:44:03.255096Z","shell.execute_reply.started":"2024-12-21T23:44:03.243779Z","shell.execute_reply":"2024-12-21T23:44:03.254166Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                                    Accuracy  \\\nRandom Forest + Count Unigram + SMOTE               0.796577   \nRandom Forest + Count Unigram + RandomUnderSampler  0.517359   \nRandom Forest + Count Unigram + SMOTETomek          0.796577   \nRandom Forest + Count Unigram + None                0.801467   \nRandom Forest + Count Bigram + SMOTE                0.783374   \nRandom Forest + Count Bigram + RandomUnderSampler   0.586797   \nRandom Forest + Count Bigram + SMOTETomek           0.783374   \nRandom Forest + Count Bigram + None                 0.791687   \nRandom Forest + TFIDF Unigram + SMOTE               0.811247   \nRandom Forest + TFIDF Unigram + RandomUnderSampler  0.498289   \nRandom Forest + TFIDF Unigram + SMOTETomek          0.811247   \nRandom Forest + TFIDF Unigram + None                0.799022   \nRandom Forest + TFIDF Bigram + SMOTE                0.797066   \nRandom Forest + TFIDF Bigram + RandomUnderSampler   0.609291   \nRandom Forest + TFIDF Bigram + SMOTETomek           0.797066   \nRandom Forest + TFIDF Bigram + None                 0.801956   \n\n                                                    Precision(Weighted)  \\\nRandom Forest + Count Unigram + SMOTE                          0.800860   \nRandom Forest + Count Unigram + RandomUnderSampler             0.721103   \nRandom Forest + Count Unigram + SMOTETomek                     0.800860   \nRandom Forest + Count Unigram + None                           0.816743   \nRandom Forest + Count Bigram + SMOTE                           0.805562   \nRandom Forest + Count Bigram + RandomUnderSampler              0.718596   \nRandom Forest + Count Bigram + SMOTETomek                      0.805562   \nRandom Forest + Count Bigram + None                            0.801924   \nRandom Forest + TFIDF Unigram + SMOTE                          0.803693   \nRandom Forest + TFIDF Unigram + RandomUnderSampler             0.724423   \nRandom Forest + TFIDF Unigram + SMOTETomek                     0.803693   \nRandom Forest + TFIDF Unigram + None                           0.810564   \nRandom Forest + TFIDF Bigram + SMOTE                           0.805049   \nRandom Forest + TFIDF Bigram + RandomUnderSampler              0.696288   \nRandom Forest + TFIDF Bigram + SMOTETomek                      0.805049   \nRandom Forest + TFIDF Bigram + None                            0.804217   \n\n                                                    Recall(Weighted)  \\\nRandom Forest + Count Unigram + SMOTE                       0.796577   \nRandom Forest + Count Unigram + RandomUnderSampler          0.517359   \nRandom Forest + Count Unigram + SMOTETomek                  0.796577   \nRandom Forest + Count Unigram + None                        0.801467   \nRandom Forest + Count Bigram + SMOTE                        0.783374   \nRandom Forest + Count Bigram + RandomUnderSampler           0.586797   \nRandom Forest + Count Bigram + SMOTETomek                   0.783374   \nRandom Forest + Count Bigram + None                         0.791687   \nRandom Forest + TFIDF Unigram + SMOTE                       0.811247   \nRandom Forest + TFIDF Unigram + RandomUnderSampler          0.498289   \nRandom Forest + TFIDF Unigram + SMOTETomek                  0.811247   \nRandom Forest + TFIDF Unigram + None                        0.799022   \nRandom Forest + TFIDF Bigram + SMOTE                        0.797066   \nRandom Forest + TFIDF Bigram + RandomUnderSampler           0.609291   \nRandom Forest + TFIDF Bigram + SMOTETomek                   0.797066   \nRandom Forest + TFIDF Bigram + None                         0.801956   \n\n                                                    F1-Score(Weighted)  \nRandom Forest + Count Unigram + SMOTE                         0.744846  \nRandom Forest + Count Unigram + RandomUnderSampler            0.552655  \nRandom Forest + Count Unigram + SMOTETomek                    0.744846  \nRandom Forest + Count Unigram + None                          0.752864  \nRandom Forest + Count Bigram + SMOTE                          0.718219  \nRandom Forest + Count Bigram + RandomUnderSampler             0.619743  \nRandom Forest + Count Bigram + SMOTETomek                     0.718219  \nRandom Forest + Count Bigram + None                           0.733897  \nRandom Forest + TFIDF Unigram + SMOTE                         0.771345  \nRandom Forest + TFIDF Unigram + RandomUnderSampler            0.533864  \nRandom Forest + TFIDF Unigram + SMOTETomek                    0.771345  \nRandom Forest + TFIDF Unigram + None                          0.749325  \nRandom Forest + TFIDF Bigram + SMOTE                          0.744939  \nRandom Forest + TFIDF Bigram + RandomUnderSampler             0.639312  \nRandom Forest + TFIDF Bigram + SMOTETomek                     0.744939  \nRandom Forest + TFIDF Bigram + None                           0.754445  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Precision(Weighted)</th>\n      <th>Recall(Weighted)</th>\n      <th>F1-Score(Weighted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Random Forest + Count Unigram + SMOTE</th>\n      <td>0.796577</td>\n      <td>0.800860</td>\n      <td>0.796577</td>\n      <td>0.744846</td>\n    </tr>\n    <tr>\n      <th>Random Forest + Count Unigram + RandomUnderSampler</th>\n      <td>0.517359</td>\n      <td>0.721103</td>\n      <td>0.517359</td>\n      <td>0.552655</td>\n    </tr>\n    <tr>\n      <th>Random Forest + Count Unigram + SMOTETomek</th>\n      <td>0.796577</td>\n      <td>0.800860</td>\n      <td>0.796577</td>\n      <td>0.744846</td>\n    </tr>\n    <tr>\n      <th>Random Forest + Count Unigram + None</th>\n      <td>0.801467</td>\n      <td>0.816743</td>\n      <td>0.801467</td>\n      <td>0.752864</td>\n    </tr>\n    <tr>\n      <th>Random Forest + Count Bigram + SMOTE</th>\n      <td>0.783374</td>\n      <td>0.805562</td>\n      <td>0.783374</td>\n      <td>0.718219</td>\n    </tr>\n    <tr>\n      <th>Random Forest + Count Bigram + RandomUnderSampler</th>\n      <td>0.586797</td>\n      <td>0.718596</td>\n      <td>0.586797</td>\n      <td>0.619743</td>\n    </tr>\n    <tr>\n      <th>Random Forest + Count Bigram + SMOTETomek</th>\n      <td>0.783374</td>\n      <td>0.805562</td>\n      <td>0.783374</td>\n      <td>0.718219</td>\n    </tr>\n    <tr>\n      <th>Random Forest + Count Bigram + None</th>\n      <td>0.791687</td>\n      <td>0.801924</td>\n      <td>0.791687</td>\n      <td>0.733897</td>\n    </tr>\n    <tr>\n      <th>Random Forest + TFIDF Unigram + SMOTE</th>\n      <td>0.811247</td>\n      <td>0.803693</td>\n      <td>0.811247</td>\n      <td>0.771345</td>\n    </tr>\n    <tr>\n      <th>Random Forest + TFIDF Unigram + RandomUnderSampler</th>\n      <td>0.498289</td>\n      <td>0.724423</td>\n      <td>0.498289</td>\n      <td>0.533864</td>\n    </tr>\n    <tr>\n      <th>Random Forest + TFIDF Unigram + SMOTETomek</th>\n      <td>0.811247</td>\n      <td>0.803693</td>\n      <td>0.811247</td>\n      <td>0.771345</td>\n    </tr>\n    <tr>\n      <th>Random Forest + TFIDF Unigram + None</th>\n      <td>0.799022</td>\n      <td>0.810564</td>\n      <td>0.799022</td>\n      <td>0.749325</td>\n    </tr>\n    <tr>\n      <th>Random Forest + TFIDF Bigram + SMOTE</th>\n      <td>0.797066</td>\n      <td>0.805049</td>\n      <td>0.797066</td>\n      <td>0.744939</td>\n    </tr>\n    <tr>\n      <th>Random Forest + TFIDF Bigram + RandomUnderSampler</th>\n      <td>0.609291</td>\n      <td>0.696288</td>\n      <td>0.609291</td>\n      <td>0.639312</td>\n    </tr>\n    <tr>\n      <th>Random Forest + TFIDF Bigram + SMOTETomek</th>\n      <td>0.797066</td>\n      <td>0.805049</td>\n      <td>0.797066</td>\n      <td>0.744939</td>\n    </tr>\n    <tr>\n      <th>Random Forest + TFIDF Bigram + None</th>\n      <td>0.801956</td>\n      <td>0.804217</td>\n      <td>0.801956</td>\n      <td>0.754445</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"model_name = \"XGBoost\"\n\nxgb_df = results_df_T.loc[\n    results_df_T.index.str.startswith(model_name)\n]\nxgb_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T23:44:15.899583Z","iopub.execute_input":"2024-12-21T23:44:15.899872Z","iopub.status.idle":"2024-12-21T23:44:15.911831Z","shell.execute_reply.started":"2024-12-21T23:44:15.899850Z","shell.execute_reply":"2024-12-21T23:44:15.910895Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                              Accuracy  Precision(Weighted)  \\\nXGBoost + Count Unigram + SMOTE               0.770171             0.741892   \nXGBoost + Count Unigram + RandomUnderSampler  0.596577             0.702937   \nXGBoost + Count Unigram + SMOTETomek          0.770171             0.741892   \nXGBoost + Count Unigram + None                0.783863             0.800883   \nXGBoost + Count Bigram + SMOTE                0.764303             0.729501   \nXGBoost + Count Bigram + RandomUnderSampler   0.595110             0.701852   \nXGBoost + Count Bigram + SMOTETomek           0.764303             0.729501   \nXGBoost + Count Bigram + None                 0.781907             0.792781   \nXGBoost + TFIDF Unigram + SMOTE               0.755501             0.727794   \nXGBoost + TFIDF Unigram + RandomUnderSampler  0.579951             0.700254   \nXGBoost + TFIDF Unigram + SMOTETomek          0.755501             0.727794   \nXGBoost + TFIDF Unigram + None                0.783863             0.787547   \nXGBoost + TFIDF Bigram + SMOTE                0.748166             0.720248   \nXGBoost + TFIDF Bigram + RandomUnderSampler   0.561858             0.679202   \nXGBoost + TFIDF Bigram + SMOTETomek           0.748166             0.720248   \nXGBoost + TFIDF Bigram + None                 0.783863             0.785993   \n\n                                              Recall(Weighted)  \\\nXGBoost + Count Unigram + SMOTE                       0.770171   \nXGBoost + Count Unigram + RandomUnderSampler          0.596577   \nXGBoost + Count Unigram + SMOTETomek                  0.770171   \nXGBoost + Count Unigram + None                        0.783863   \nXGBoost + Count Bigram + SMOTE                        0.764303   \nXGBoost + Count Bigram + RandomUnderSampler           0.595110   \nXGBoost + Count Bigram + SMOTETomek                   0.764303   \nXGBoost + Count Bigram + None                         0.781907   \nXGBoost + TFIDF Unigram + SMOTE                       0.755501   \nXGBoost + TFIDF Unigram + RandomUnderSampler          0.579951   \nXGBoost + TFIDF Unigram + SMOTETomek                  0.755501   \nXGBoost + TFIDF Unigram + None                        0.783863   \nXGBoost + TFIDF Bigram + SMOTE                        0.748166   \nXGBoost + TFIDF Bigram + RandomUnderSampler           0.561858   \nXGBoost + TFIDF Bigram + SMOTETomek                   0.748166   \nXGBoost + TFIDF Bigram + None                         0.783863   \n\n                                              F1-Score(Weighted)  \nXGBoost + Count Unigram + SMOTE                         0.718056  \nXGBoost + Count Unigram + RandomUnderSampler            0.634198  \nXGBoost + Count Unigram + SMOTETomek                    0.718056  \nXGBoost + Count Unigram + None                          0.721403  \nXGBoost + Count Bigram + SMOTE                          0.708268  \nXGBoost + Count Bigram + RandomUnderSampler             0.632524  \nXGBoost + Count Bigram + SMOTETomek                     0.708268  \nXGBoost + Count Bigram + None                           0.719445  \nXGBoost + TFIDF Unigram + SMOTE                         0.733640  \nXGBoost + TFIDF Unigram + RandomUnderSampler            0.621413  \nXGBoost + TFIDF Unigram + SMOTETomek                    0.733640  \nXGBoost + TFIDF Unigram + None                          0.724917  \nXGBoost + TFIDF Bigram + SMOTE                          0.727535  \nXGBoost + TFIDF Bigram + RandomUnderSampler             0.603378  \nXGBoost + TFIDF Bigram + SMOTETomek                     0.727535  \nXGBoost + TFIDF Bigram + None                           0.727087  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Precision(Weighted)</th>\n      <th>Recall(Weighted)</th>\n      <th>F1-Score(Weighted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>XGBoost + Count Unigram + SMOTE</th>\n      <td>0.770171</td>\n      <td>0.741892</td>\n      <td>0.770171</td>\n      <td>0.718056</td>\n    </tr>\n    <tr>\n      <th>XGBoost + Count Unigram + RandomUnderSampler</th>\n      <td>0.596577</td>\n      <td>0.702937</td>\n      <td>0.596577</td>\n      <td>0.634198</td>\n    </tr>\n    <tr>\n      <th>XGBoost + Count Unigram + SMOTETomek</th>\n      <td>0.770171</td>\n      <td>0.741892</td>\n      <td>0.770171</td>\n      <td>0.718056</td>\n    </tr>\n    <tr>\n      <th>XGBoost + Count Unigram + None</th>\n      <td>0.783863</td>\n      <td>0.800883</td>\n      <td>0.783863</td>\n      <td>0.721403</td>\n    </tr>\n    <tr>\n      <th>XGBoost + Count Bigram + SMOTE</th>\n      <td>0.764303</td>\n      <td>0.729501</td>\n      <td>0.764303</td>\n      <td>0.708268</td>\n    </tr>\n    <tr>\n      <th>XGBoost + Count Bigram + RandomUnderSampler</th>\n      <td>0.595110</td>\n      <td>0.701852</td>\n      <td>0.595110</td>\n      <td>0.632524</td>\n    </tr>\n    <tr>\n      <th>XGBoost + Count Bigram + SMOTETomek</th>\n      <td>0.764303</td>\n      <td>0.729501</td>\n      <td>0.764303</td>\n      <td>0.708268</td>\n    </tr>\n    <tr>\n      <th>XGBoost + Count Bigram + None</th>\n      <td>0.781907</td>\n      <td>0.792781</td>\n      <td>0.781907</td>\n      <td>0.719445</td>\n    </tr>\n    <tr>\n      <th>XGBoost + TFIDF Unigram + SMOTE</th>\n      <td>0.755501</td>\n      <td>0.727794</td>\n      <td>0.755501</td>\n      <td>0.733640</td>\n    </tr>\n    <tr>\n      <th>XGBoost + TFIDF Unigram + RandomUnderSampler</th>\n      <td>0.579951</td>\n      <td>0.700254</td>\n      <td>0.579951</td>\n      <td>0.621413</td>\n    </tr>\n    <tr>\n      <th>XGBoost + TFIDF Unigram + SMOTETomek</th>\n      <td>0.755501</td>\n      <td>0.727794</td>\n      <td>0.755501</td>\n      <td>0.733640</td>\n    </tr>\n    <tr>\n      <th>XGBoost + TFIDF Unigram + None</th>\n      <td>0.783863</td>\n      <td>0.787547</td>\n      <td>0.783863</td>\n      <td>0.724917</td>\n    </tr>\n    <tr>\n      <th>XGBoost + TFIDF Bigram + SMOTE</th>\n      <td>0.748166</td>\n      <td>0.720248</td>\n      <td>0.748166</td>\n      <td>0.727535</td>\n    </tr>\n    <tr>\n      <th>XGBoost + TFIDF Bigram + RandomUnderSampler</th>\n      <td>0.561858</td>\n      <td>0.679202</td>\n      <td>0.561858</td>\n      <td>0.603378</td>\n    </tr>\n    <tr>\n      <th>XGBoost + TFIDF Bigram + SMOTETomek</th>\n      <td>0.748166</td>\n      <td>0.720248</td>\n      <td>0.748166</td>\n      <td>0.727535</td>\n    </tr>\n    <tr>\n      <th>XGBoost + TFIDF Bigram + None</th>\n      <td>0.783863</td>\n      <td>0.785993</td>\n      <td>0.783863</td>\n      <td>0.727087</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"model_name = \"LightGBM\"\n\nlgb_df = results_df_T.loc[\n    results_df_T.index.str.startswith(model_name)\n]\nlgb_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T23:44:16.302960Z","iopub.execute_input":"2024-12-21T23:44:16.303261Z","iopub.status.idle":"2024-12-21T23:44:16.314072Z","shell.execute_reply.started":"2024-12-21T23:44:16.303236Z","shell.execute_reply":"2024-12-21T23:44:16.313380Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                               Accuracy  Precision(Weighted)  \\\nLightGBM + Count Unigram + SMOTE               0.804890             0.799305   \nLightGBM + Count Unigram + RandomUnderSampler  0.453301             0.714625   \nLightGBM + Count Unigram + SMOTETomek          0.804890             0.799305   \nLightGBM + Count Unigram + None                0.804401             0.803307   \nLightGBM + Count Bigram + SMOTE                0.805379             0.807511   \nLightGBM + Count Bigram + RandomUnderSampler   0.453301             0.714625   \nLightGBM + Count Bigram + SMOTETomek           0.805379             0.807511   \nLightGBM + Count Bigram + None                 0.803912             0.801731   \nLightGBM + TFIDF Unigram + SMOTE               0.801956             0.788630   \nLightGBM + TFIDF Unigram + RandomUnderSampler  0.454768             0.698966   \nLightGBM + TFIDF Unigram + SMOTETomek          0.801956             0.788630   \nLightGBM + TFIDF Unigram + None                0.813692             0.809081   \nLightGBM + TFIDF Bigram + SMOTE                0.798533             0.785666   \nLightGBM + TFIDF Bigram + RandomUnderSampler   0.453790             0.689582   \nLightGBM + TFIDF Bigram + SMOTETomek           0.798533             0.785666   \nLightGBM + TFIDF Bigram + None                 0.809291             0.796262   \n\n                                               Recall(Weighted)  \\\nLightGBM + Count Unigram + SMOTE                       0.804890   \nLightGBM + Count Unigram + RandomUnderSampler          0.453301   \nLightGBM + Count Unigram + SMOTETomek                  0.804890   \nLightGBM + Count Unigram + None                        0.804401   \nLightGBM + Count Bigram + SMOTE                        0.805379   \nLightGBM + Count Bigram + RandomUnderSampler           0.453301   \nLightGBM + Count Bigram + SMOTETomek                   0.805379   \nLightGBM + Count Bigram + None                         0.803912   \nLightGBM + TFIDF Unigram + SMOTE                       0.801956   \nLightGBM + TFIDF Unigram + RandomUnderSampler          0.454768   \nLightGBM + TFIDF Unigram + SMOTETomek                  0.801956   \nLightGBM + TFIDF Unigram + None                        0.813692   \nLightGBM + TFIDF Bigram + SMOTE                        0.798533   \nLightGBM + TFIDF Bigram + RandomUnderSampler           0.453790   \nLightGBM + TFIDF Bigram + SMOTETomek                   0.798533   \nLightGBM + TFIDF Bigram + None                         0.809291   \n\n                                               F1-Score(Weighted)  \nLightGBM + Count Unigram + SMOTE                         0.769342  \nLightGBM + Count Unigram + RandomUnderSampler            0.527475  \nLightGBM + Count Unigram + SMOTETomek                    0.769342  \nLightGBM + Count Unigram + None                          0.765372  \nLightGBM + Count Bigram + SMOTE                          0.765767  \nLightGBM + Count Bigram + RandomUnderSampler             0.527475  \nLightGBM + Count Bigram + SMOTETomek                     0.765767  \nLightGBM + Count Bigram + None                           0.765011  \nLightGBM + TFIDF Unigram + SMOTE                         0.790207  \nLightGBM + TFIDF Unigram + RandomUnderSampler            0.528255  \nLightGBM + TFIDF Unigram + SMOTETomek                    0.790207  \nLightGBM + TFIDF Unigram + None                          0.782758  \nLightGBM + TFIDF Bigram + SMOTE                          0.787682  \nLightGBM + TFIDF Bigram + RandomUnderSampler             0.513997  \nLightGBM + TFIDF Bigram + SMOTETomek                     0.787682  \nLightGBM + TFIDF Bigram + None                           0.783693  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Precision(Weighted)</th>\n      <th>Recall(Weighted)</th>\n      <th>F1-Score(Weighted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LightGBM + Count Unigram + SMOTE</th>\n      <td>0.804890</td>\n      <td>0.799305</td>\n      <td>0.804890</td>\n      <td>0.769342</td>\n    </tr>\n    <tr>\n      <th>LightGBM + Count Unigram + RandomUnderSampler</th>\n      <td>0.453301</td>\n      <td>0.714625</td>\n      <td>0.453301</td>\n      <td>0.527475</td>\n    </tr>\n    <tr>\n      <th>LightGBM + Count Unigram + SMOTETomek</th>\n      <td>0.804890</td>\n      <td>0.799305</td>\n      <td>0.804890</td>\n      <td>0.769342</td>\n    </tr>\n    <tr>\n      <th>LightGBM + Count Unigram + None</th>\n      <td>0.804401</td>\n      <td>0.803307</td>\n      <td>0.804401</td>\n      <td>0.765372</td>\n    </tr>\n    <tr>\n      <th>LightGBM + Count Bigram + SMOTE</th>\n      <td>0.805379</td>\n      <td>0.807511</td>\n      <td>0.805379</td>\n      <td>0.765767</td>\n    </tr>\n    <tr>\n      <th>LightGBM + Count Bigram + RandomUnderSampler</th>\n      <td>0.453301</td>\n      <td>0.714625</td>\n      <td>0.453301</td>\n      <td>0.527475</td>\n    </tr>\n    <tr>\n      <th>LightGBM + Count Bigram + SMOTETomek</th>\n      <td>0.805379</td>\n      <td>0.807511</td>\n      <td>0.805379</td>\n      <td>0.765767</td>\n    </tr>\n    <tr>\n      <th>LightGBM + Count Bigram + None</th>\n      <td>0.803912</td>\n      <td>0.801731</td>\n      <td>0.803912</td>\n      <td>0.765011</td>\n    </tr>\n    <tr>\n      <th>LightGBM + TFIDF Unigram + SMOTE</th>\n      <td>0.801956</td>\n      <td>0.788630</td>\n      <td>0.801956</td>\n      <td>0.790207</td>\n    </tr>\n    <tr>\n      <th>LightGBM + TFIDF Unigram + RandomUnderSampler</th>\n      <td>0.454768</td>\n      <td>0.698966</td>\n      <td>0.454768</td>\n      <td>0.528255</td>\n    </tr>\n    <tr>\n      <th>LightGBM + TFIDF Unigram + SMOTETomek</th>\n      <td>0.801956</td>\n      <td>0.788630</td>\n      <td>0.801956</td>\n      <td>0.790207</td>\n    </tr>\n    <tr>\n      <th>LightGBM + TFIDF Unigram + None</th>\n      <td>0.813692</td>\n      <td>0.809081</td>\n      <td>0.813692</td>\n      <td>0.782758</td>\n    </tr>\n    <tr>\n      <th>LightGBM + TFIDF Bigram + SMOTE</th>\n      <td>0.798533</td>\n      <td>0.785666</td>\n      <td>0.798533</td>\n      <td>0.787682</td>\n    </tr>\n    <tr>\n      <th>LightGBM + TFIDF Bigram + RandomUnderSampler</th>\n      <td>0.453790</td>\n      <td>0.689582</td>\n      <td>0.453790</td>\n      <td>0.513997</td>\n    </tr>\n    <tr>\n      <th>LightGBM + TFIDF Bigram + SMOTETomek</th>\n      <td>0.798533</td>\n      <td>0.785666</td>\n      <td>0.798533</td>\n      <td>0.787682</td>\n    </tr>\n    <tr>\n      <th>LightGBM + TFIDF Bigram + None</th>\n      <td>0.809291</td>\n      <td>0.796262</td>\n      <td>0.809291</td>\n      <td>0.783693</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"model_name = \"ANN\"\n\nann_df = results_df_T.loc[\n    results_df_T.index.str.startswith(model_name)\n]\nann_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T10:18:18.781578Z","iopub.execute_input":"2024-12-22T10:18:18.781859Z","iopub.status.idle":"2024-12-22T10:18:18.793169Z","shell.execute_reply.started":"2024-12-22T10:18:18.781838Z","shell.execute_reply":"2024-12-22T10:18:18.792372Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                          Accuracy  Precision(Weighted)  \\\nANN + Count Unigram + SMOTE               0.800978             0.784159   \nANN + Count Unigram + RandomUnderSampler  0.513447             0.733535   \nANN + Count Unigram + SMOTETomek          0.800978             0.784159   \nANN + Count Unigram + None                0.829829             0.810568   \nANN + Count Bigram + SMOTE                0.811247             0.793758   \nANN + Count Bigram + RandomUnderSampler   0.452812             0.745935   \nANN + Count Bigram + SMOTETomek           0.811247             0.793758   \nANN + Count Bigram + None                 0.821516             0.802874   \nANN + TFIDF Unigram + SMOTE               0.820538             0.805989   \nANN + TFIDF Unigram + RandomUnderSampler  0.533007             0.727373   \nANN + TFIDF Unigram + SMOTETomek          0.820538             0.805989   \nANN + TFIDF Unigram + None                0.828851             0.809240   \nANN + TFIDF Bigram + SMOTE                0.836675             0.818582   \nANN + TFIDF Bigram + RandomUnderSampler   0.581907             0.724609   \nANN + TFIDF Bigram + SMOTETomek           0.836675             0.818582   \nANN + TFIDF Bigram + None                 0.843032             0.833466   \n\n                                          Recall(Weighted)  F1-Score(Weighted)  \nANN + Count Unigram + SMOTE                       0.800978            0.791574  \nANN + Count Unigram + RandomUnderSampler          0.513447            0.583239  \nANN + Count Unigram + SMOTETomek                  0.800978            0.791574  \nANN + Count Unigram + None                        0.829829            0.814260  \nANN + Count Bigram + SMOTE                        0.811247            0.801415  \nANN + Count Bigram + RandomUnderSampler           0.452812            0.496272  \nANN + Count Bigram + SMOTETomek                   0.811247            0.801415  \nANN + Count Bigram + None                         0.821516            0.809259  \nANN + TFIDF Unigram + SMOTE                       0.820538            0.812640  \nANN + TFIDF Unigram + RandomUnderSampler          0.533007            0.600276  \nANN + TFIDF Unigram + SMOTETomek                  0.820538            0.812640  \nANN + TFIDF Unigram + None                        0.828851            0.814022  \nANN + TFIDF Bigram + SMOTE                        0.836675            0.823718  \nANN + TFIDF Bigram + RandomUnderSampler           0.581907            0.641348  \nANN + TFIDF Bigram + SMOTETomek                   0.836675            0.823718  \nANN + TFIDF Bigram + None                         0.843032            0.821917  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Precision(Weighted)</th>\n      <th>Recall(Weighted)</th>\n      <th>F1-Score(Weighted)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ANN + Count Unigram + SMOTE</th>\n      <td>0.800978</td>\n      <td>0.784159</td>\n      <td>0.800978</td>\n      <td>0.791574</td>\n    </tr>\n    <tr>\n      <th>ANN + Count Unigram + RandomUnderSampler</th>\n      <td>0.513447</td>\n      <td>0.733535</td>\n      <td>0.513447</td>\n      <td>0.583239</td>\n    </tr>\n    <tr>\n      <th>ANN + Count Unigram + SMOTETomek</th>\n      <td>0.800978</td>\n      <td>0.784159</td>\n      <td>0.800978</td>\n      <td>0.791574</td>\n    </tr>\n    <tr>\n      <th>ANN + Count Unigram + None</th>\n      <td>0.829829</td>\n      <td>0.810568</td>\n      <td>0.829829</td>\n      <td>0.814260</td>\n    </tr>\n    <tr>\n      <th>ANN + Count Bigram + SMOTE</th>\n      <td>0.811247</td>\n      <td>0.793758</td>\n      <td>0.811247</td>\n      <td>0.801415</td>\n    </tr>\n    <tr>\n      <th>ANN + Count Bigram + RandomUnderSampler</th>\n      <td>0.452812</td>\n      <td>0.745935</td>\n      <td>0.452812</td>\n      <td>0.496272</td>\n    </tr>\n    <tr>\n      <th>ANN + Count Bigram + SMOTETomek</th>\n      <td>0.811247</td>\n      <td>0.793758</td>\n      <td>0.811247</td>\n      <td>0.801415</td>\n    </tr>\n    <tr>\n      <th>ANN + Count Bigram + None</th>\n      <td>0.821516</td>\n      <td>0.802874</td>\n      <td>0.821516</td>\n      <td>0.809259</td>\n    </tr>\n    <tr>\n      <th>ANN + TFIDF Unigram + SMOTE</th>\n      <td>0.820538</td>\n      <td>0.805989</td>\n      <td>0.820538</td>\n      <td>0.812640</td>\n    </tr>\n    <tr>\n      <th>ANN + TFIDF Unigram + RandomUnderSampler</th>\n      <td>0.533007</td>\n      <td>0.727373</td>\n      <td>0.533007</td>\n      <td>0.600276</td>\n    </tr>\n    <tr>\n      <th>ANN + TFIDF Unigram + SMOTETomek</th>\n      <td>0.820538</td>\n      <td>0.805989</td>\n      <td>0.820538</td>\n      <td>0.812640</td>\n    </tr>\n    <tr>\n      <th>ANN + TFIDF Unigram + None</th>\n      <td>0.828851</td>\n      <td>0.809240</td>\n      <td>0.828851</td>\n      <td>0.814022</td>\n    </tr>\n    <tr>\n      <th>ANN + TFIDF Bigram + SMOTE</th>\n      <td>0.836675</td>\n      <td>0.818582</td>\n      <td>0.836675</td>\n      <td>0.823718</td>\n    </tr>\n    <tr>\n      <th>ANN + TFIDF Bigram + RandomUnderSampler</th>\n      <td>0.581907</td>\n      <td>0.724609</td>\n      <td>0.581907</td>\n      <td>0.641348</td>\n    </tr>\n    <tr>\n      <th>ANN + TFIDF Bigram + SMOTETomek</th>\n      <td>0.836675</td>\n      <td>0.818582</td>\n      <td>0.836675</td>\n      <td>0.823718</td>\n    </tr>\n    <tr>\n      <th>ANN + TFIDF Bigram + None</th>\n      <td>0.843032</td>\n      <td>0.833466</td>\n      <td>0.843032</td>\n      <td>0.821917</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}